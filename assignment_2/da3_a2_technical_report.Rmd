---
title: "Data Analysis 3 - Assignment 2"
subtitle: "Airbnb Price Prediction - Montreal, Canada"
author: "Xibei Chen"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: "readable"
    #toc: true
    #toc_depth: 2
    #toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, error=FALSE)
```

```{r, include=FALSE}
# Clear environment
rm(list=ls())

# Load packages
library(tidyverse)
library(data.table)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(cowplot)
library(modelsummary)
library(fixest)
library(ggthemes)
library(gridExtra)
library(rpart)
library(rattle)
library(rpart.plot)
library(ranger)
library(pdp)
library(gbm)
library(ggpubr)
library(kableExtra)

# Load raw data
data <- read_csv("/Users/xibei/Documents/2021CEU/DA3/listings_montreal_20211211_cleaned.csv") %>%
  mutate_if(is.character, factor) 
```


# Introduction
The purpose of this project is to help a company set price to their new apartments that are not yet on the rental market. The company is operating small and mid-size apartments hosting 2-6 guests in Montreal. To help them set reasonable price,  my approach is to analyze the data of airbnb listings in Montreal on 11th Dec 2021 (the most recent date available) from [Inside Airbnb](http://insideairbnb.com/get-the-data.html) (a website providing fairly cleaned web scrapped data on listings throughout all the major cities of the world) and then I build several price prediction models based on the data with methods, such as OLS, Lasso, and Random Forest. Eventually with prediction performance, interpretability, external validity etc. taken into consideration, I will decide which prediction model to use for helping the company set price to their apartments.

# Data Preparation 
Data preparation includes mainly three parts: Data Cleaning, Label Engineearing, and Feature Engineearing. (Please find more details on [Cleaner R script](https://github.com/xibei-chen/DA3/blob/main/assignment_2/montreal_20211211_cleaner.R))

### Data Cleaning
Filter Observations:

* With valid Airbnb's unique identifier for the listing;
* The maximum capacity is between 2 and 6;
* Room type is entire apartment or flat;
* Property type is entire place (boat and RV excluded).

Change Format:

* Remove percentage signs fro, variable host_response_rate and host_acceptance_rate;
* Remove dollar signs from variable price;
* Format binary variables from "T/F" to "1/0" for host_is_superhost, host_has_profile_pic, host_identity_verified, instant_bookable, has_availability;
* Clean bedrooms_text, extract only the number of bedrooms;
* Transform host_since, first_review and last_review from date to to days difference to today

Handle missing values:

* Drop observations with missing value of target variable price;
* Drop observations if only with a few missing values (2 or 3 observations);
* Drop variables with too many missing values: license;
* Drop variables with missing values but can be substituted by other variable: neighbourhood and host_neighbourhood can be explained by neighbourhood_cleansed which is also a better cleaned variable;
* Impute missing value of number of beds with accommodates, as both explain the number of guests;
* Impute missing value of number of bedrooms with the median of number of bedrooms, as it is integer value.
* Create flag variables for variables with many missing values but I still want to keep in the data, in case they are important for predicting price, and replace missing value with 0: review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value, reviews_per_month, host_response_rate, host_acceptance_rate, first_review_to_today, last_review_to_today.

### Label Engineering & EDA
Our target variable is the daily price in CA$, which is very straightforward. 

##### Descriptive Statistics of Daily Price
```{r, echo=FALSE}
datasummary( price ~ Mean + Median + SD+ Min + P25 + P75 + Max + N, data = data ) %>% 
             kable_styling(latex_options = c("HOLD_position","scale_down"))
```
Mean and Median is not too far way from each other, but Mean is larger than Median, indicating there are some extreme values at right side, which is verified by the following graph of distribution of daily price. For now I decide not to exclude them, because there is no proof that they are errors.

```{r, echo=FALSE, fig.width=6, fig.height=4,fig.align='center'}
# Check distribution of target variable - price
ggplot( data = data, aes( x = price) )  +
        geom_density(color='#2a9d8f',fill='#3cc5a3', alpha=0.5) +
        theme_minimal() +
        labs( x = NULL, y = NULL,
              title = 'Distribution of Daily Price (CA$)') +
        theme( panel.grid.minor.x = element_blank(), 
               plot.title = element_text( size = 10, face = "bold", hjust = 0.5 ) ) +
        theme( legend.position = "none" ) 
```


### Feature Engineering
I create many dummy variables for amenities and host_verifications. And I created following variable groups for further analysis about variable importance. (Please find more details on [Define Variable Groups](https://github.com/xibei-chen/DA3/blob/main/assignment_2/define_var_groups.R))

* amenities includes amenities_general, home_appliance_brands, beauty_care_brands, baby_children_friendly, digital_entertainment, outdoor_facilities;
* host_verifications.

And I put the following symbols to indicate what type the varible is.

* "d_": dummy variables, includes also "d_missing" which means flag variable for missing value;
* "f_": factor variables;
* "n_": numeric variables, includes also "n_p" for percentage variables and "n_dt" for days difference to today.

I have also added following features to capture potential non-linear patterns.

* squared_number_of_reviews;
* cubic_number_of_reviews;
* ln_host_since_to_today.

Interaction terms below have also been created for modelling with OLS and Lasso, with interaction graphs shown below. We can see that for different property types, the patterns between price and if pets allowed, and if there is pool change. In addition for different neighbourhoods, the patterns between price and if there is free parking and if there is lake access also change. (Here I did not create interaction terms with amenities, because there are so many, it would be super hard to compute because of the lack of computing power)

* f_property_type*d_pets_allowed;
* f_property_type*d_pool;
* f_neighbourhood_cleansed*d_free_parking;
* f_neighbourhood_cleansed*d_lake_access.     

```{r, include=FALSE}
# Source a function (Credits to Agoston) -> to plot interaction terms
source("/Users/xibei/Documents/2021CEU/DA3/price_diff_by_variables2.R")

# Look up property type with pets allowed and pool
p1 <- price_diff_by_variables2(data, "f_property_type", "d_pets_allowed", "Property type", "Pets Allowed") + theme(axis.text.x = element_text(angle = 45, hjust=1))
p2 <- price_diff_by_variables2(data, "f_property_type", "d_pool", "Property type", "Pool") + theme(axis.text.x = element_text(angle = 45, hjust=1))
# Patterns change

# Look up neighbourhood with beachfront and lake access
p3 <- price_diff_by_variables2(data, "f_neighbourhood_cleansed", "d_free_parking", "Neighbourhood", "Free Parking")+ theme(axis.text.x = element_text(angle = 45, hjust=1))
p4 <- price_diff_by_variables2(data, "f_neighbourhood_cleansed", "d_lake_access", "Neighbourhood", "Lake Access")+ theme(axis.text.x = element_text(angle = 45, hjust=1))
# Patterns change
```

```{r, echo=FALSE, fig.align='center', fig.dim=c(10,6)}
ggarrange(p1, p2, ncol = 2, nrow = 1)
```

```{r, echo=FALSE, fig.align='center', fig.dim=c(8,6)}
p3
```

```{r, echo=FALSE, fig.align='center', fig.dim=c(8,6)}
p4
```

# Modelling

### Mange different samples:
Create a holdout set with 20% of observations by random sampling, and the left 80% would be used as work set.
```{r, include=FALSE}
# Manage samples:
# Create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data))

# Set the random number generator to make results reproducible
set.seed(20220210)

# Create ids:
holdout_ids <- sample( seq_len( nrow( data ) ) , size = smp_size )
data$holdout <- 0
data$holdout[holdout_ids] <- 1

# Hold-out set Set
data_holdout <- data %>% filter(holdout == 1)

# Working data set
data_work <- data %>% filter(holdout == 0)
```

### Create multiple predictor levels: 
```{r, include=FALSE}
# Source from pre-created variable groups
source("/Users/xibei/Documents/2021CEU/DA3/define_var_groups.R")
```

* basic_lev: f_neighbourhood_cleansed, f_property_type, n_accommodates, n_bedrooms, n_beds, n_dt_host_since_to_today;
* basic_add_lev: n_minimum_nights, n_maximum_nights, d_has_availability, n_availability_30, d_instant_bookable;
* reviews_lev: n_number_of_reviews, n_review_scores_rating, n_review_scores_cleanliness, n_review_scores_value;
* poly_log_lev: n_squared_number_of_reviews, n_cubic_number_of_reviews, n_ln_host_since_to_today;
* amenities_general;
* X1: f_property_type\*d_pets_allowed, f_property_type\*d_pool
* X2: f_neighbourhood_cleansed\*d_free_parking, f_neighbourhood_cleansed\*d_lake_access

```{r, include=FALSE}
# Basic levels
basic_lev  <- c("f_neighbourhood_cleansed", "f_property_type", "n_accommodates","n_bedrooms","n_beds", "n_dt_host_since_to_today")

# Basic Add levels
basic_add_lev <- c("n_minimum_nights", "n_maximum_nights", "d_has_availability", "n_availability_30", "d_instant_bookable")

# Reviews levels
reviews_lev <- c("n_number_of_reviews","n_review_scores_rating","n_review_scores_cleanliness", "n_review_scores_value")

# Polynomial and log-level 
poly_log_lev <- c("n_squared_number_of_reviews", "n_cubic_number_of_reviews", "n_ln_host_since_to_today")

# Amenities general levels - I grouped already previously
# amenities_general

# Interaction terms
X1  <- c("f_property_type*d_pets_allowed",  "f_property_type*d_pool")
X2  <- c("f_neighbourhood_cleansed*d_free_parking", "f_neighbourhood_cleansed*d_lake_access")
```


### (1) OLS 
I build following 7 models with increasing model complexity.

* Model 1: n_accommodates;
* Model 2: basic_lev;
* Model 3: basic_lev, basic_add_lev;
* Model 4: basic_lev, basic_add_lev, reviews_lev, poly_log_lev;
* Model 5: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1;
* Model 6: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1, X2;
* Model 7: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1, X2, amenities_general.
```{r, include=FALSE}
# Create models 1-8 from simple to complex
modellev1 <- " ~ n_accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1,X2),collapse = " + "))
modellev7 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1,X2, amenities_general),collapse = " + "))


# Utilize the Working data set:
# Estimate measures on the whole working sample (R2,BIC,RMSE)
# Do K-fold cross validation to get proper Test RMSE

# Do everything within a for-loop
# K = 5
k_folds <- 5
# Define seed value
seed_val <- 20220210

# Do the iteration
for ( i in 1:7 ){
        print(paste0( "Estimating model: " ,i ))
        # Get the model name
        model_name <-  paste0("modellev",i)
        model_pretty_name <- paste0("M",i,"")
        # Specify the formula
        yvar <- "price"
        xvars <- eval(parse(text = model_name))
        formula <- formula(paste0(yvar,xvars))
        
        # Estimate model on the whole sample
        model_work_data <- feols( formula , data = data_work , vcov='hetero' )
        #  and get the summary statistics
        fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
        BIC <- fs$bic
        r2  <- fs$r2
        rmse_train <- fs$rmse
        ncoeff <- length( model_work_data$coefficients )
        
        # Do the k-fold estimation
        set.seed(seed_val)
        cv_i <- train( formula, data_work, method = "lm", 
                       trControl = trainControl(method = "cv", number = k_folds))
        rmse_test <- mean( cv_i$resample$RMSE )
        
        # Save the results
        model_add <- tibble(Model=model_pretty_name, Coefficients=ncoeff,
                            R_squared=r2, BIC = BIC, 
                            Training_RMSE = rmse_train, Test_RMSE = rmse_test )
        if ( i == 1 ){
                model_results <- model_add
        } else{
                model_results <- rbind( model_results , model_add )
        }
}
```
```{r}
kable(model_results, booktabs = TRUE, caption = 'OLS Model 1-7 Performance Summary (5-fold CV)') %>% kable_styling(latex_options = c("hold_position","scale_down"))
```

From the above summary table, we can see that in terms of BIC, M3 with 57 coefficients is the best. However, M7 with 152 coefficients is the best according to R-squared, Training RMSE and Test RMSE, compared to M6, M7 commits roughly 1.2$ Test RMSE, which is quite a improvement.

```{r, echo=FALSE, fig.align='center' }
# RMSE training vs test graph
colors = c("Training RMSE"="#2a9d8f","Test RMSE" = "#e85d04")
ggplot( data = model_results, aes( x = factor( Coefficients ) , group = 1 ) )+
        geom_line(aes( y = Training_RMSE , color = 'Training RMSE'), size = 1 ) +
        geom_line(aes( y = Test_RMSE , color = 'Test RMSE') , size = 1 )+
        labs(y='RMSE',x='Number of coefficients',color = "")+
        scale_color_manual(values = colors)+
        theme_bw()+
        theme(legend.position="top")
```
From the above graph, we can see that with the increase of OLS model complexity and number of coefficients, both Training and Test RMSE are dropping, which implies that our model might not yet overfit the training data.

### (2) Lasso - Modelling
```{r}
# Set lasso tuning parameters:
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
# Take OLS model 7
formula <- formula(paste0("price", paste(modellev7, collapse = " + ")))

# Run LASSO
set.seed(seed_val)
lasso_model <- caret::train(formula,
                            data = data_work,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)
```

For Lasso modelling, we still use the OLS M7 predictors. The elasticnet mixing parameter can be set with 0 ≤ α ≤ 1. We can let machine run different α to find the optimal one. However, to reduce the computing time, here I set alpha=1, which is the lasso penalty. I let machine run lamda from 0.05 to 1 by 0.05, to get the optimal. 

```{r, fig.align='center'}
# Check the output
#lasso_model
# Check th optimal lambda parameter 0.2
#lasso_model$bestTune$lambda
# Check the RMSE curve
plot(lasso_model, col = "#2a9d8f")
```
```{r, include=FALSE}
# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
        as.matrix() %>%
        as.data.frame() %>%
        rownames_to_column(var = "variable") %>%
        rename(coefficient = `s1`)  # the column has a name "1", to be renamed

#print(lasso_coeffs)

# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
        filter(coefficient!=0)
#print(nrow(lasso_coeffs_nz))
# Lasso picks 106 coefficients
```


```{r}
# Get the RMSE of the Lasso model 
# Compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
        filter(lambda == lasso_model$bestTune$lambda) 

# Create an auxilary tibble
lasso_add <- tibble(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    R_squared=lasso_fitstats$Rsquared, BIC = NA, 
                    Training_RMSE = NA, Test_RMSE = lasso_fitstats$RMSE )
# Add it to final results
model_results_lasso <- rbind( model_results , lasso_add )
```

```{r}
kable(model_results_lasso, booktabs = TRUE, caption = 'OLS and Lasso Performance Summary (5-fold CV)') %>% kable_styling(latex_options = c("hold_position","scale_down"))
```

It turns out the optimal lambda parameter lies at `r lasso_model$bestTune$lambda`, as shown in the above graph. Besides, we can get the number of coefficients that Lasso picked, which is `r nrow(lasso_coeffs_nz)`. From the summary table, we can tell that Lasso gets us slight better Test_RMSE than OLS M7 by around 0.2$.

```{r, include=FALSE}
# Diagnostics
# Evaluate performance on the hold-out sample
# Re-run Model 7 on the work data
m7 <- feols( formula(paste0("price",modellev7)) , data = data_work, vcov = 'hetero' )

# Make prediction for the hold-out sample with M7 and Lasso
m7_p <- predict( m7 , newdata = data_holdout )
mL_p <- predict( lasso_model , newdata = data_holdout )

# Calculate the RMSE on hold-out sample
m7_rmse <- RMSE(m7_p,data_holdout$price)
mL_rmse <- RMSE(mL_p,data_holdout$price)

# Create a table
sum <- rbind(m7_rmse,mL_rmse)
rownames(sum) <- c('OLS M7','LASSO')
colnames(sum) <- c('RMSE on hold-out sample')
# Lasso also won Model 7 on hold-out sample by around 0.2$
```
```{r, echo=FALSE}
kable(sum, booktabs = TRUE) %>% kable_styling(full_width = F, latex_options = c("hold_position","scale_down"))
```

I also evaluated the prediction performance of both OLS M7 and Lasso on our hold-out sample. As we can see from above table, Lasso also wins OLS M7, by roughly 0.4$.

### (2) Lasso - Diagnostics

```{r, echo=FALSE, fig.align='center'} 
# Predicted vs Actual prices
data_holdout$predLp <- mL_p

ggplot( data_holdout , aes( y = price , x = predLp ) ) +
        geom_point( size = 1 , color = '#2a9d8f' , alpha=0.4) +
        geom_abline( intercept = 0, slope = 1, size = 1, color = '#e85d04' , alpha=0.8, linetype = 'dashed') +
        xlim(-1,max(data_holdout$price))+
        ylim(-1,max(data_holdout$price))+
        labs(x='Predicted Daily Price (CA$)',y='Acutal Daily Price (CA$)')+
        theme_minimal()
```
I have also created the above graph to visualize how Lasso performs prediction on hold-out sample. We can see that Lasso does not perform prediction well at extreme values of actual price, even though it is better than OLS models in general.

### (3) Random Forest
```{r, include=FALSE}
predictors_all <- colnames(data)
elements_2_remove <- c("price", "holdout")
predictors_all <- predictors_all[!(predictors_all %in% elements_2_remove)]

# Do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)

# Set tuning, 177 predictors, so set 13 variables per split, and arbitrarily set 50 min size per terminal node.
tune_grid <- expand.grid(
        .mtry = c(13),
        .splitrule = "variance",
        .min.node.size = c(50)
)

set.seed(seed_val)

rf_model <- train(
                formula(paste0("price ~", paste0(predictors_all, collapse = " + "))),
                data = data_work,
                method = "ranger",
                trControl = train_control,
                tuneGrid = tune_grid,
                importance = "impurity"
        )


```

Another method for prediction is regression tree, which can capture interactions and non-linearities automatically. However, it is prone to overfit data, even after pruning. Therefore, I choose to use Random Forest directly here after OLS and Lasso. With the method of Bootstrap Aggregation, 500 trees are created based on similar but not the exact same samples, and then the mean of predicted price is calculated. As I have `r length(predictors_all)` predictors in total, I arbitrarily set for each split, only randomly picked 13 variables (closest to square root of `r length(predictors_all)`) are taken into consideration. In this way, the trees are decorrelated kept independent from each other, and more chance is given to all the predictors. Both bootstrapping and decorrelating trees mean using random sets of information (observations and predictors). By tuning I also arbitrarily set 50 observations for minimum size per terminal node to avoid each tree overfitting to some extent.

### Model Performace Comparison
```{r, echo=FALSE}
M7predictors <- c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, amenities_general, X1, X2)

set.seed(20220210)
ols_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_work,
                method = "lm",
                trControl = train_control
        )
# ols_model_coeffs <-  ols_model$finalModel$coefficients
# ols_model_coeffs_df <- data.frame(
#         "variable" = names(ols_model_coeffs),
#         "ols_coefficient" = ols_model_coeffs
# ) %>%
#         mutate(variable = gsub("`","",variable))

# set.seed(20220210)
# lasso_model <- train(
#                 formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
#                 data = data_work,
#                 method = "glmnet",
#                 preProcess = c("center", "scale"),
#                 tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
#                 trControl = train_control
#         )
# 
# lasso_coeffs <- coef(
#         lasso_model$finalModel,
#         lasso_model$bestTune$lambda) %>%
#         as.matrix() %>%
#         as.data.frame() %>%
#         rownames_to_column(var = "variable") %>%
#         rename(lasso_coefficient = `s1`)
# 
# lasso_coeffs_non_null <- lasso_coeffs[!lasso_coeffs$lasso_coefficient == 0,]
# 
# regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_non_null, by = "variable", all=TRUE)


# Compare these models
final_models <-
        list("OLS (M7 predictors)" = ols_model,
             "LASSO (M7 predictors)" = lasso_model,
             "Random forest  (all predictors)" = rf_model)

results <- resamples(final_models) %>% summary()
#results

# Model selection is carried out on this CV RMSE
result_cv <- imap(final_models, ~{
        mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("CV RMSE" = ".")
# Random forest is the best

# Evaluate preferred model on the holdout set
result_holdout <- map(final_models, ~{
        RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("Holdout RMSE" = ".")

result_summary <- result_cv %>% cbind(result_holdout)
# Random forest is the best

kable(result_summary, booktabs = TRUE) %>% kable_styling(full_width= F, latex_options = c("hold_position","scale_down"))
```

From the above summary table, we can tell that Random Forest does better prediction than both OLS and Lasso. In term of Holdout RMSE, Random forest outperforms Lasso by around 3$, which is quite a good improvement.



# Robustness Check 
```{r, include=FALSE}
# Define extreme values 75th percentile + 1.5 * IQR = 226
extreme_price <- quantile(data$price, 0.75) + 1.5*(quantile(data$price, 0.75)-quantile(data$price, 0.25))
```

Even though random Forest does quite a good prediction job compared to OLS and Lasso, the RMSE on the hold-out set is still quite large, above 60\$. Recall that Lasso does poor job at prediction extreme values, leads me to think about whether excluding apartments with extreme values of price could make us get a model with better prediction. Therefore, I decided to exclude apartments with price higher than 75th percentile plus 1.5*IQR which is `r extreme_price`\$ to rerun the above three models. 
```{r, echo=FALSE}
# Since we are a company operating small and mid-size apartments hosting 2-6 guests, we are not supposed to be extreme values normally.
# So I decided to exclude the flat whose price is above 75th percentile + 1.5*IQR
data_ne <- data %>% filter(price <= extreme_price)


# Manage samples:
# Create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data_ne))

# Set the random number generator to make results reproducible
set.seed(20220210)

# Create ids:
holdout_ids <- sample( seq_len( nrow( data_ne) ) , size = smp_size )
data_ne$holdout <- 0
data_ne$holdout[holdout_ids] <- 1

# Hold-out set Set
data_ne_holdout <- data_ne %>% filter(holdout == 1)

# Working data set
data_ne_work <- data_ne %>% filter(holdout == 0)

# OLS
set.seed(20220210)
ne_ols_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_ne_work,
                method = "lm",
                trControl = train_control
        )

# Lasso
set.seed(20220210)
ne_lasso_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_ne_work,
                method = "glmnet",
                preProcess = c("center", "scale"),
                tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
                trControl = train_control
        )


# Random forest
set.seed(20220210)

ne_rf_model <- train(
                formula(paste0("price ~", paste0(predictors_all, collapse = " + "))),
                data = data_ne_work,
                method = "ranger",
                trControl = train_control,
                tuneGrid = tune_grid,
                importance = "impurity"
        )



# Compare these models
ne_final_models <-
        list("OLS (M7 predictors)" = ne_ols_model,
             "LASSO (M7 predictors)" = ne_lasso_model,
             "Random forest  (all predictors)" = ne_rf_model)

ne_results <- resamples(ne_final_models) %>% summary()
#ne_results

# Model selection is carried out on this CV RMSE
ne_result_cv <- imap(ne_final_models, ~{
        mean(ne_results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("CV RMSE (without EV)" = ".")

# Evaluate preferred model on the holdout set
ne_result_holdout <- map(ne_final_models, ~{
        RMSE(predict(.x, newdata = data_ne_holdout), data_ne_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("Holdout RMSE (without EV)" = ".")

ne_result_summary <- ne_result_cv %>% cbind(ne_result_holdout)
# Random forest is the best
kable(ne_result_summary, booktabs = TRUE) %>% kable_styling(full_width= F, latex_options = c("hold_position","scale_down"))
```
We can tell from the above table that without extreme values taken into consideration, Random forest is still the best for both CV RMSE and Holdout RMSE. What is new is that the RMSE is only half of the RMSE compared to when we did not exclude extreme values earlier. This is huge improvement regarding prediction performance. Therefore, if the properties our company is operating has no unusual features, for example neighbor of celebrities, collections of famous paintings in the house, or any other luxurious facilities, if they are just normal properties, I would use the data without apartments with extreme values of price to train a Random Forest model for much better prediction. 


# External Validity
Random Forest gives better performance than OLS and Lasso at predicting quantitative target variable, but this comes at the cost. It is a black box, we do not have coefficients for hand-picked variables to interpret. Therefore, it is very hard to tell what patterns of association between daily price and the predictor variables look like. Besides, we are not aware of which variables are more important for the prediction. Patterns of association and variable important are extremely necessary for us to evaluate external validity. Therefore, I applied some diagnostic tools such as variable importance, partial dependence and performance across subsamples to dig deeper into the random forest to find out important variable and the pattern of association between them and price, and how performance varies among different subsamples. Here I keep using the data without extreme values of price.

### Variable Importance

Variable Importance is a sum of gains in terms of MSE reduction by splits involving the variable.
```{r, echo=FALSE, fig.align='center', fig.dim=c(10,6)}
#### Variable importance plot
ne_rf_model_var_imp <- ranger::importance(ne_rf_model$finalModel)/1000 # scaled it down by 1000
ne_rf_model_var_imp_df <-
        data.frame(varname = names(ne_rf_model_var_imp),imp = ne_rf_model_var_imp) %>%
        mutate(varname = gsub("f_neighbourhood_cleansed", "District:", varname) ) %>%
        mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
        arrange(desc(imp)) %>%
        mutate(imp_percentage = imp/sum(imp))

# Full variable importance plot
#plot(varImp(ne_rf_model)) # too many variables, not clear

# Zoom in a bit
cutoff = 50
vi1 <- ggplot(ne_rf_model_var_imp_df[ne_rf_model_var_imp_df$imp>cutoff,],
       aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1.5) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha=0.6, size=1) +
        ylab("Importance (Percent)") +
        xlab("Variable Name") +
        coord_flip() +
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal() +
        theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
              axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))

# Top 10 important variable
vi2<-ggplot(ne_rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha = 0.6, size=0.75) +
        ylab("Importance (Percent)") +
        xlab("Variable Name") +
        coord_flip() +
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal()

ggarrange(vi1, vi2, ncol = 2, nrow = 1)
```

Since there will be too many variables on full variable importance plot, and it would be impossible to see clearly. Here I only set cut off to only show us some variables on the top in terms of variable importance on the left side. And I zoom in to see only top 10 on the right side. Here we can see that number of bedrooms, the maximum capacity and the number of bathrooms are the top 3, which can pass our sanity check. Plus, from the graph on the left we can see there are a lot variable belonging to the same group, such as availability, minimum and maximum nights, etc. To better understand how different variable groups are in terms of importance, I grouped variables in the same category together, such as availability, minimum and maximum nights, general amenities, beauty care brands, digital entertainment, host verifications, etc.
```{r, echo=FALSE, fig.align='center'}
# Grouped variable importance
availabilities_varnames <- grep("n_availability",colnames(data), value = TRUE)
minmax_nights_varnames <- grep("n_m",colnames(data), value = TRUE)
groups <- list(minmax_nights = minmax_nights_varnames,
               availability_days = availabilities_varnames,
               amenities_general = amenities_general,
               baby_children_friendly = baby_children_friendly,
               beauty_care_brands = beauty_care_brands,
               digital_entertainment = digital_entertainment,
               home_appliance_brands = home_appliance_brands,
               host_verifications = host_verifications,
               outdoor_facilities = outdoor_facilities,
               n_bathrooms = "n_bathrooms",
               n_accommodates = "n_accommodates",
               n_bedrooms = "n_bedrooms", 
               f_property_type = "f_property_type",
               f_neighbourhood_cleansed = "f_neighbourhood_cleansed"
               )

# Need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
        var.imp <- as.matrix(sapply(groups, function(g) {
                sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
        }))
        colnames(var.imp) <- "MeanDecreaseGini"
        return(var.imp)
}

ne_rf_model_var_imp_grouped <- group.importance(ne_rf_model$finalModel, groups)
ne_rf_model_var_imp_grouped_df <- data.frame(varname = rownames(ne_rf_model_var_imp_grouped),
                                            imp = ne_rf_model_var_imp_grouped[,1])  %>%
        mutate(imp_percentage = imp/sum(imp))

ggplot(ne_rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha=0.6, size=0.7) +
        ylab("Importance (Percent)") +   xlab("Variable Name") +
        coord_flip() +
        # expand=c(0,0),
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal()

```
Before grouping variables, dishwasher is the only one factor among amenities that lies on the top of variable importance plot. However, after grouping, we can see that the sum of importance of all the general amenities variables is actually quite large. And it is the same for minimum and maximum nights and availability variables. They are more importance than the original top 3 after grouping together the variables. Another thing we can see is that although we barely saw individual variables appears at the top variable importance chart, after grouping up, variable groups such as host verification, baby children friendly, outdoor facilities, digital entertainment and beauty care brands do effect price prediction.


### Partial Dependence
Next I examine the pattern of association between price and some of the predictor variables. I picked two variables that I would like to know more about and check the pattern: the number of guests to accommodate and property type.
```{r, echo=FALSE, fig.align='center'}
# Partial Dependence Plots 
# Number of accommodates
pdp_n_acc <- pdp::partial(ne_rf_model, pred.var = "n_accommodates", 
                          pred.grid = distinct_(data_ne_holdout, "n_accommodates"), 
                          train = data_ne_work)

pdp_n_acc %>%
        autoplot( ) +
        geom_line(color='#2a9d8f', size=1) +
        geom_point(color='#e85d04', size=2) +
        ylab("Predicted price") +
        xlab("Accommodates (persons)") +
        scale_x_continuous(limit=c(2,6), breaks=seq(2,6,1))+
        theme_minimal()
```

```{r, echo=FALSE, fig.align='center'}
# Property type
pdp_n_propertytype <- pdp::partial(ne_rf_model, pred.var = "f_property_type", 
                               pred.grid = distinct_(data_ne_holdout, "f_property_type"), 
                               train = data_ne_work)
pdp_n_propertytype %>%
        autoplot( ) +
        geom_point(color='#e85d04', size=4) +
        ylab("Predicted price") +
        xlab("Property type") +
        scale_y_continuous(limits=c(95,105), breaks=seq(95,105, by=5)) +
        theme_minimal()+
        theme(axis.text.x=element_text(angle=45,hjust=1)) 

```
The above plots suggests a pretty linear and positive relationship between predicted price and number of guests. In terms of property types, entire townhouse, entire loft and entire residential home are the top 3 expensive types, whereas entire rental unit is the cheapest type.

### Performance on Subsamples
To further investigate the performance of the random forest model, I also look at subsamples by three predictor variables: apartment size, property types and districts. For apartment size, I divide apartments into two group, ones with 2 or 3 guests capacity, and other ones with 4-6 guests. For property types, I arbitrarily pick two that I am more interested in, namely Entire condo and Entire loft. For districts, I choose 5 districts that are located in the inner city, where most investment properties are located. Only calculating RMSE is not meaningful, because RMSE is based on price difference, hence is by definition lower when prices are lower. Therefore, I compared RMSE values relative to the corresponding mean predicted price. 
```{r, echo=FALSE}
# Subsample performance
data_ne_holdout_w_prediction <- data_ne_holdout %>%
        mutate(predicted_price = predict(ne_rf_model, newdata = data_ne_holdout))

# Create nice summary table of heterogeneity
a <- data_ne_holdout_w_prediction %>%
        mutate(is_low_size = ifelse(n_accommodates <= 3, "Small apt", "Medium apt")) %>%
        group_by(is_low_size) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = RMSE(predicted_price, price) / mean(price)
        )

b <- data_ne_holdout_w_prediction %>%
        filter(f_neighbourhood_cleansed %in% c("Ville-Marie", "Le Plateau-Mont-Royal",
                                               "Le Sud-Ouest", "Côte-des-Neiges-Notre-Dame-de-Grâce",
                                               "Westmount")) %>%
        group_by(f_neighbourhood_cleansed) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = rmse / mean_price
        )

c <- data_ne_holdout_w_prediction %>%
        filter(f_property_type %in% c("Entire condominium (condo)", "Entire loft")) %>%
        group_by(f_property_type) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = rmse / mean_price
        )

d <- data_ne_holdout_w_prediction %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = RMSE(predicted_price, price) / mean(price)
        )

# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("District", "", "", "")
result_subsamples <- rbind(line2, a, line1, c, line3, b, d) %>%
        transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
                  `RMSE/price` = as.numeric(`RMSE/price`))

options(knitr.kable.NA = '')

kable(result_subsamples, booktabs = TRUE) %>% kable_styling(full_width= F, latex_options = c("hold_position","scale_down"))
```
We can see from the above table that prices of small size apartments are slightly harder to predict than medium size apartments. However, for condo and loft there is not much difference regarding predictive performance. Besides, Côte-des-Neiges-Notre-Dame-de-Grâce and Westmount, the two districts at the west part of city center, are harder than other inner city districts to predict price. 

# Conclusion
Among 7 OLS models, Lasso and Random Forest, Random Forest has the best prediction performance. By robustness check, I found out by excluding apartments with extreme value prices, prediction performance can be improved significantly. So given that the properties of our company are normal apartments without extraordinary features, I would use Random Forest on data without extreme values of price for prediction. However, we still keep in mind that depending on the location and features the properties of our company, we might have different prediction performance.
