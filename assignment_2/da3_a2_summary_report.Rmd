---
title: "Airbnb Price Prediction - Montreal, Canada"
author: "Xibei Chen"
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: "left=1cm,right=1cm,top=1.2cm,bottom=1.2cm"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, error=FALSE)
```

```{r, include=FALSE}
# Clear environment
rm(list=ls())

# Load packages
library(tidyverse)
library(data.table)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(cowplot)
library(modelsummary)
library(fixest)
library(ggthemes)
library(gridExtra)
library(rpart)
library(rattle)
library(rpart.plot)
library(ranger)
library(pdp)
library(gbm)
library(ggpubr)
library(kableExtra)

# Load raw data
data <- read_csv("/Users/xibei/Documents/2021CEU/DA3/listings_montreal_20211211_cleaned.csv") %>%
  mutate_if(is.character, factor) 
```


## Executive Summary
The purpose of this project is to help a company set price to their new apartments that are not yet on the rental market. The company is operating small and mid-size apartments hosting 2-6 guests in Montreal. To help them set reasonable price, I analyzed the data of airbnb listings in Montreal on 11th Dec 2021 (the most recent date available) from Inside Airbnb, and build several price prediction models with OLS, Lasso, and Random Forest. While OLS and Lasso are helpful to investigate the patterns of association with interpreting coefficients of variables, Random Forest generates the best prediction performance. Therefore, the final model of choice in my project is Random Forest. Through robustness check, I found the prediction performance can be improved significantly if extreme values are excluded, given that the properties of the company do not have unusual fancy features. Eventually, as Random Forest is a black-box model, I also applied diagnostic tools to evaluate variable importance, partial dependence and performance on subsamples to help the company make better decisions regarding setting price for apartments with different features and investing in what features to generate higher profit.

## Data Preparation 
Data preparation includes mainly three parts: Data Cleaning, Label Engineering, and Feature Engineering. For Data Cleaning, I filtered observations meeting the our business logic, for example maximum capacity is between 2 and 6. and room type is entire apartment or flat. I have also changed format of some variables and handled missing values with different methods.


```{r, echo=FALSE}
datasummary( price ~ Mean + Median + SD+ Min + P25 + P75 + Max + N, data = data) %>% kable_styling(latex_options = c("HOLD_position","scale_down"), font_size = 4)
```
In terms of label engineering, our target variable is the daily price in CA Dollars. From the above descriptive statistics of daily price, we can see the Mean and Median are not too far way from each other, but Mean is larger than Median, indicating there are some extreme values at right side. For now I decided not to exclude them, as they are not proved to be errors. 

##### Feature Engineering
I create many dummy variables for amenities and host_verifications, and the variable groups respectively for further dignostics. And I put the symbols such as "d_", "f_" and "n_" to indicate what type the variable is, dummy, factor or numeric. I have also added some features with polynomial or log-level to capture potential non-linear patterns. Furthermore, interaction terms have also been created for modelling with OLS and Lasso, to capture different patterns of association between price and if pets allowed or if there is pool for different property types, and different patterns between price and if there is free parking or if there is lake access for different neighbourhoods. (Here I did not create interaction terms with amenities, because there are so many, it would be super hard to compute because of the lack of computing power)


## Modelling

##### Preparation

Firstly, I manged different samples by creating a holdout set with 20% of observations by random sampling, and the left 80% would be used as work set. Secondly, multiple predictor levels are created: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, amenities_general, and two interactions levels X1 and X2. Moreover, number of folds for cross validation is set as 5 for all the models.

```{r, include=FALSE}
# Manage samples:
# Create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data))

# Set the random number generator to make results reproducible
set.seed(20220210)

# Create ids:
holdout_ids <- sample( seq_len( nrow( data ) ) , size = smp_size )
data$holdout <- 0
data$holdout[holdout_ids] <- 1

# Hold-out set Set
data_holdout <- data %>% filter(holdout == 1)

# Working data set
data_work <- data %>% filter(holdout == 0)
```


```{r, include=FALSE}
# Source from pre-created variable groups
source("/Users/xibei/Documents/2021CEU/DA3/define_var_groups.R")
```

```{r, include=FALSE}
# Basic levels
basic_lev  <- c("f_neighbourhood_cleansed", "f_property_type", "n_accommodates","n_bedrooms","n_beds", "n_dt_host_since_to_today")

# Basic Add levels
basic_add_lev <- c("n_minimum_nights", "n_maximum_nights", "d_has_availability", "n_availability_30", "d_instant_bookable")

# Reviews levels
reviews_lev <- c("n_number_of_reviews","n_review_scores_rating","n_review_scores_cleanliness", "n_review_scores_value")

# Polynomial and log-level 
poly_log_lev <- c("n_squared_number_of_reviews", "n_cubic_number_of_reviews", "n_ln_host_since_to_today")

# Amenities general levels - I grouped already previously
# amenities_general

# Interaction terms
X1  <- c("f_property_type*d_pets_allowed",  "f_property_type*d_pool")
X2  <- c("f_neighbourhood_cleansed*d_free_parking", "f_neighbourhood_cleansed*d_lake_access")
```

\newpage
##### (1) OLS & (2) Lasso
I built 7 models with increasing model complexity, in terms of number of predictor levels and number of coefficients. The most complex model 7 has all the predictor levels and interaction terms created previously.
```{r, include=FALSE}
# Create models 1-8 from simple to complex
modellev1 <- " ~ n_accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1,X2),collapse = " + "))
modellev7 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1,X2, amenities_general),collapse = " + "))


# Utilize the Working data set:
# Estimate measures on the whole working sample (R2,BIC,RMSE)
# Do K-fold cross validation to get proper Test RMSE

# Do everything within a for-loop
# K = 5
k_folds <- 5
# Define seed value
seed_val <- 20220210

# Do the iteration
for ( i in 1:7 ){
        print(paste0( "Estimating model: " ,i ))
        # Get the model name
        model_name <-  paste0("modellev",i)
        model_pretty_name <- paste0("M",i,"")
        # Specify the formula
        yvar <- "price"
        xvars <- eval(parse(text = model_name))
        formula <- formula(paste0(yvar,xvars))
        
        # Estimate model on the whole sample
        model_work_data <- feols( formula , data = data_work , vcov='hetero' )
        #  and get the summary statistics
        fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
        BIC <- fs$bic
        r2  <- fs$r2
        rmse_train <- fs$rmse
        ncoeff <- length( model_work_data$coefficients )
        
        # Do the k-fold estimation
        set.seed(seed_val)
        cv_i <- train( formula, data_work, method = "lm", 
                       trControl = trainControl(method = "cv", number = k_folds))
        rmse_test <- mean( cv_i$resample$RMSE )
        
        # Save the results
        model_add <- tibble(Model=model_pretty_name, Coefficients=ncoeff,
                            R_squared=r2, BIC = BIC, 
                            Training_RMSE = rmse_train, Test_RMSE = rmse_test )
        if ( i == 1 ){
                model_results <- model_add
        } else{
                model_results <- rbind( model_results , model_add )
        }
}
```


```{r, include=FALSE}
# Set lasso tuning parameters:
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
# Take OLS model 7
formula <- formula(paste0("price", paste(modellev7, collapse = " + ")))

# Run LASSO
set.seed(seed_val)
lasso_model <- caret::train(formula,
                            data = data_work,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)
```


```{r, include=FALSE}
# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
        as.matrix() %>%
        as.data.frame() %>%
        rownames_to_column(var = "variable") %>%
        rename(coefficient = `s1`)  # the column has a name "1", to be renamed

#print(lasso_coeffs)

# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
        filter(coefficient!=0)
#print(nrow(lasso_coeffs_nz))
# Lasso picks 106 coefficients
```


```{r, include=FALSE}
# Get the RMSE of the Lasso model 
# Compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
        filter(lambda == lasso_model$bestTune$lambda) 

# Create an auxilary tibble
lasso_add <- tibble(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    R_squared=lasso_fitstats$Rsquared, BIC = NA, 
                    Training_RMSE = NA, Test_RMSE = lasso_fitstats$RMSE )
# Add it to final results
model_results_lasso <- rbind( model_results , lasso_add )
```

```{r, echo=FALSE}
kable(model_results_lasso, booktabs = TRUE, linesep = "", caption = 'OLS and Lasso Performance Summary (5-fold CV)') %>% kable_styling(full_width= F, latex_options = c("hold_position","scale_down"), font_size = 4)
```


```{r, include=FALSE}
# Diagnostics
# Evaluate performance on the hold-out sample
# Re-run Model 7 on the work data
m7 <- feols( formula(paste0("price",modellev7)) , data = data_work, vcov = 'hetero' )

# Make prediction for the hold-out sample with M7 and Lasso
m7_p <- predict( m7 , newdata = data_holdout )
mL_p <- predict( lasso_model , newdata = data_holdout )

# Calculate the RMSE on hold-out sample
m7_rmse <- RMSE(m7_p,data_holdout$price)
mL_rmse <- RMSE(mL_p,data_holdout$price)

# Create a table
sum <- rbind(m7_rmse,mL_rmse)
rownames(sum) <- c('OLS M7','LASSO')
colnames(sum) <- c('RMSE on hold-out sample')
# Lasso also won Model 7 on hold-out sample by around 0.2$
```

For Lasso modelling, I still used the OLS M7 predictors. From the above summary table, we see M7 with 152 coefficients is the best among OLS Models. However, Lasso with `r nrow(lasso_coeffs_nz)` coefficients picked from M7 predictors gets us slightly better Test_RMSE than OLS M7 by around 0.2\$. I also evaluated the prediction performance of both OLS M7 and Lasso on our hold-out sample. OLS M7 with `r round(m7_rmse, 2)`\$, and Lasso `r round(mL_rmse,2)`\$. Hence, Lasso also wins OLS M7 on hold-out set, by roughly 0.4\$.

```{r, echo=FALSE, fig.dim=c(3,2), fig.align="center"} 
# Predicted vs Actual prices
data_holdout$predLp <- mL_p

ggplot( data_holdout , aes( y = price , x = predLp ) ) +
        geom_point( size = 1 , color = '#2a9d8f' , alpha=0.4) +
        geom_abline( intercept = 0, slope = 1, size = 1, color = '#e85d04' , alpha=0.8, linetype = 'dashed') +
        xlim(-1,max(data_holdout$price))+
        ylim(-1,max(data_holdout$price))+
        labs(x='Predicted Daily Price (CA$)',y='Acutal Daily Price (CA$)')+
        theme_minimal()
```
The above graph visualizes how Lasso performs prediction on hold-out sample. We can see that even though it is better than OLS models in general, Lasso does not perform prediction well at extreme values of actual price.

##### (3) Random Forest
```{r, include=FALSE}
predictors_all <- colnames(data)
elements_2_remove <- c("price", "holdout")
predictors_all <- predictors_all[!(predictors_all %in% elements_2_remove)]

# Do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)

# Set tuning, 177 predictors, so set 13 variables per split, and arbitrarily set 50 min size per terminal node.
tune_grid <- expand.grid(
        .mtry = c(13),
        .splitrule = "variance",
        .min.node.size = c(50)
)

set.seed(seed_val)

rf_model <- train(
                formula(paste0("price ~", paste0(predictors_all, collapse = " + "))),
                data = data_work,
                method = "ranger",
                trControl = train_control,
                tuneGrid = tune_grid,
                importance = "impurity"
        )


```

Another method for prediction is regression tree, which can capture interactions and non-linearities automatically. However, it is prone to overfit data even after pruning. Therefore, I choose to use Random Forest directly here after OLS and Lasso. With the method of Bootstrap Aggregation, 500 trees are created based on similar but not the exact same samples. As I have `r length(predictors_all)` predictors in total, I arbitrarily set for each split only randomly picked 13 variables (closest to square root of `r length(predictors_all)`) are taken into consideration. In this way, the trees are decorrelated and kept independent from each other, and more chance is given to all the predictors. By tuning I also arbitrarily set 50 observations for minimum size per terminal node to avoid overfitting each tree. It turns out Random Forest does better prediction than both OLS and Lasso. In term of Holdout RMSE, Random forest outperforms Lasso by roughly 3\$, which is quite a good improvement.

```{r, include=FALSE}
M7predictors <- c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, amenities_general, X1, X2)

set.seed(20220210)
ols_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_work,
                method = "lm",
                trControl = train_control
        )
# ols_model_coeffs <-  ols_model$finalModel$coefficients
# ols_model_coeffs_df <- data.frame(
#         "variable" = names(ols_model_coeffs),
#         "ols_coefficient" = ols_model_coeffs
# ) %>%
#         mutate(variable = gsub("`","",variable))

# set.seed(20220210)
# lasso_model <- train(
#                 formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
#                 data = data_work,
#                 method = "glmnet",
#                 preProcess = c("center", "scale"),
#                 tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
#                 trControl = train_control
#         )
# 
# lasso_coeffs <- coef(
#         lasso_model$finalModel,
#         lasso_model$bestTune$lambda) %>%
#         as.matrix() %>%
#         as.data.frame() %>%
#         rownames_to_column(var = "variable") %>%
#         rename(lasso_coefficient = `s1`)
# 
# lasso_coeffs_non_null <- lasso_coeffs[!lasso_coeffs$lasso_coefficient == 0,]
# 
# regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_non_null, by = "variable", all=TRUE)


# Compare these models
final_models <-
        list("OLS (M7 predictors)" = ols_model,
             "LASSO (M7 predictors)" = lasso_model,
             "Random forest  (all predictors)" = rf_model)

results <- resamples(final_models) %>% summary()
#results

# Model selection is carried out on this CV RMSE
result_cv <- imap(final_models, ~{
        mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("CV RMSE" = ".")
# Random forest is the best

# Evaluate preferred model on the holdout set
result_holdout <- map(final_models, ~{
        RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("Holdout RMSE" = ".")

result_summary <- result_cv %>% cbind(result_holdout)
# Random forest is the best
```




## Robustness Check 
```{r, include=FALSE}
# Define extreme values 75th percentile + 1.5 * IQR = 226
extreme_price <- quantile(data$price, 0.75) + 1.5*(quantile(data$price, 0.75)-quantile(data$price, 0.25))
```

Even though random Forest does the best prediction compared to OLS and Lasso, the Holdout RMSE is still quite large, above 60\$. Recalling that Lasso does poor job at prediction extreme values, leads me to try excluding apartments with price higher than 75th percentile plus 1.5*IQR which is `r extreme_price`\$, and to rerun the above three models and re-evaluate the performance.
```{r, echo=FALSE}
# Since we are a company operating small and mid-size apartments hosting 2-6 guests, we are not supposed to be extreme values normally.
# So I decided to exclude the flat whose price is above 75th percentile + 1.5*IQR
data_ne <- data %>% filter(price <= extreme_price)


# Manage samples:
# Create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data_ne))

# Set the random number generator to make results reproducible
set.seed(20220210)

# Create ids:
holdout_ids <- sample( seq_len( nrow( data_ne) ) , size = smp_size )
data_ne$holdout <- 0
data_ne$holdout[holdout_ids] <- 1

# Hold-out set Set
data_ne_holdout <- data_ne %>% filter(holdout == 1)

# Working data set
data_ne_work <- data_ne %>% filter(holdout == 0)

# OLS
set.seed(20220210)
ne_ols_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_ne_work,
                method = "lm",
                trControl = train_control
        )

# Lasso
set.seed(20220210)
ne_lasso_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_ne_work,
                method = "glmnet",
                preProcess = c("center", "scale"),
                tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
                trControl = train_control
        )


# Random forest
set.seed(20220210)

ne_rf_model <- train(
                formula(paste0("price ~", paste0(predictors_all, collapse = " + "))),
                data = data_ne_work,
                method = "ranger",
                trControl = train_control,
                tuneGrid = tune_grid,
                importance = "impurity"
        )



# Compare these models
ne_final_models <-
        list("OLS (M7 predictors)" = ne_ols_model,
             "LASSO (M7 predictors)" = ne_lasso_model,
             "Random forest  (all predictors)" = ne_rf_model)

ne_results <- resamples(ne_final_models) %>% summary()
#ne_results

# Model selection is carried out on this CV RMSE
ne_result_cv <- imap(ne_final_models, ~{
        mean(ne_results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("CV RMSE (without EV)" = ".")

# Evaluate preferred model on the holdout set
ne_result_holdout <- map(ne_final_models, ~{
        RMSE(predict(.x, newdata = data_ne_holdout), data_ne_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("Holdout RMSE (without EV)" = ".")

ne_result_summary <- ne_result_cv %>% cbind(ne_result_holdout)

all_result_summary <- result_summary %>% cbind(ne_result_summary)
# Random forest is the best
kable(all_result_summary, booktabs = TRUE) %>% kable_styling(full_width= F, latex_options = c("hold_position","scale_down"))
```
We can tell from the below table that without extreme values, Random forest is still the best for both CV RMSE and Holdout RMSE, but only half of the RMSE when we did not exclude extreme values. This is huge improvement regarding prediction performance. Therefore, if the properties our company is operating are just normal properties and have no unusual fancy features, like neighbor of celebrities, collections of famous paintings, or any other luxurious facilities, I would use the data without apartments with extreme values of price to train a Random Forest model for much better prediction. 


## External Validity
Random Forest gives better prediction performance than OLS and Lasso at the cost of being a black box model, we do not have coefficients to interpret. Therefore, I applied some diagnostic tools to dig deeper into the random forest to find out important variables and the patterns of association between them and price, and how prediction performance varies among different subsamples.

##### Variable Importance

```{r, echo=FALSE, fig.align='center', fig.dim=c(10,2.5)}
#### Variable importance plot
ne_rf_model_var_imp <- ranger::importance(ne_rf_model$finalModel)/1000 # scaled it down by 1000
ne_rf_model_var_imp_df <-
        data.frame(varname = names(ne_rf_model_var_imp),imp = ne_rf_model_var_imp) %>%
        mutate(varname = gsub("f_neighbourhood_cleansed", "District:", varname) ) %>%
        mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
        arrange(desc(imp)) %>%
        mutate(imp_percentage = imp/sum(imp))

# Full variable importance plot
#plot(varImp(ne_rf_model)) # too many variables, not clear

# Zoom in a bit
cutoff = 50
vi1 <- ggplot(ne_rf_model_var_imp_df[ne_rf_model_var_imp_df$imp>cutoff,],
       aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1.5) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha=0.6, size=1) +
        ylab("Importance (Percent)") +
        xlab("Variable Name") +
        coord_flip() +
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal() +
        theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
              axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))

# Top 10 important variable
vi2<-ggplot(ne_rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha = 0.6, size=0.75) +
        ylab("Importance (Percent)") +
        xlab("Variable Name") +
        coord_flip() +
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal()

# Grouped variable importance
availabilities_varnames <- grep("n_availability",colnames(data), value = TRUE)
minmax_nights_varnames <- grep("n_m",colnames(data), value = TRUE)
groups <- list(minmax_nights = minmax_nights_varnames,
               availability_days = availabilities_varnames,
               amenities_general = amenities_general,
               baby_children_friendly = baby_children_friendly,
               beauty_care_brands = beauty_care_brands,
               digital_entertainment = digital_entertainment,
               home_appliance_brands = home_appliance_brands,
               host_verifications = host_verifications,
               outdoor_facilities = outdoor_facilities,
               n_bathrooms = "n_bathrooms",
               n_accommodates = "n_accommodates",
               n_bedrooms = "n_bedrooms", 
               f_property_type = "f_property_type",
               f_neighbourhood_cleansed = "f_neighbourhood_cleansed"
               )

# Need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
        var.imp <- as.matrix(sapply(groups, function(g) {
                sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
        }))
        colnames(var.imp) <- "MeanDecreaseGini"
        return(var.imp)
}

ne_rf_model_var_imp_grouped <- group.importance(ne_rf_model$finalModel, groups)
ne_rf_model_var_imp_grouped_df <- data.frame(varname = rownames(ne_rf_model_var_imp_grouped),
                                            imp = ne_rf_model_var_imp_grouped[,1])  %>%
        mutate(imp_percentage = imp/sum(imp))

vi3 <- ggplot(ne_rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha=0.6, size=0.7) +
        ylab("Importance (Percent)") +   xlab("Variable Groups Name") +
        coord_flip() +
        # expand=c(0,0),
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal()

ggarrange(vi2, vi3, ncol = 2, nrow = 1)
```

Here for clear visualization, I only zoomed in to see the top 10 on the left side. Number of bedrooms, the maximum capacity and number of bathrooms are the top 3 important variables, which can pass our sanity check. On the right side, after grouping, the sum of importance of all the general amenities is actually quite high, the same for minimum and maximum nights and availability variables. They are of more importance than the original top 3 after grouping together the variables. Plus, variable groups such as host verification, baby children friendly, outdoor facilities, digital entertainment and beauty care brands do have quite some effect on price prediction as groups as well, which is very hard to be discovered without grouping variables.


##### Partial Dependence

```{r, include=FALSE}
# Partial Dependence Plots 
# Number of accommodates
pdp_n_acc <- pdp::partial(ne_rf_model, pred.var = "n_accommodates", 
                          pred.grid = distinct_(data_ne_holdout, "n_accommodates"), 
                          train = data_ne_work)

# #pdp_n_acc %>%
#         autoplot( ) +
#         geom_line(color='#2a9d8f', size=1) +
#         geom_point(color='#e85d04', size=2) +
#         ylab("Predicted price") +
#         xlab("Accommodates (persons)") +
#         scale_x_continuous(limit=c(2,6), breaks=seq(2,6,1))+
#         theme_minimal()
```

```{r, include=FALSE}
# Property type
pdp_n_propertytype <- pdp::partial(ne_rf_model, pred.var = "f_property_type", 
                               pred.grid = distinct_(data_ne_holdout, "f_property_type"), 
                               train = data_ne_work)
# pdp_n_propertytype %>%
#         autoplot( ) +
#         geom_point(color='#e85d04', size=4) +
#         ylab("Predicted price") +
#         xlab("Property type") +
#         scale_y_continuous(limits=c(95,105), breaks=seq(95,105, by=5)) +
#         theme_minimal()+
#         theme(axis.text.x=element_text(angle=45,hjust=1)) 

```
Next I examined the pattern of association between price and two variables that I would like to know more about: the number of guests to accommodate and property type. It turns out there is a pretty linear and positive relationship between predicted price and number of guests. In terms of property types, entire townhouse, entire loft and entire residential home are the top 3 expensive types, whereas entire rental unit is the cheapest.

##### Performance on Subsamples
I also looked at subsamples by three predictor variables: apartment size, property types and districts. For apartment size, I divide apartments into two groups,  small with 2-3 guests and medium with 4-6 guests. It turns out prices of small size apartments are slightly harder to predict than medium size apartments. For property types, I arbitrarily picked two that I am more interested in, Entire condo and Entire loft, where there is not much difference regarding predictive performance. For districts, I chose 5 inner city districts, where I assume most investment properties are located. It turns out Côte-des-Neiges-Notre-Dame-de-Grâce and Westmount, the two districts on the west side of city center are harder than the rest to predict price. 
```{r, include=FALSE}
# Subsample performance
data_ne_holdout_w_prediction <- data_ne_holdout %>%
        mutate(predicted_price = predict(ne_rf_model, newdata = data_ne_holdout))

# Create nice summary table of heterogeneity
a <- data_ne_holdout_w_prediction %>%
        mutate(is_low_size = ifelse(n_accommodates <= 3, "Small apt", "Medium apt")) %>%
        group_by(is_low_size) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = RMSE(predicted_price, price) / mean(price)
        )

b <- data_ne_holdout_w_prediction %>%
        filter(f_neighbourhood_cleansed %in% c("Ville-Marie", "Le Plateau-Mont-Royal",
                                               "Le Sud-Ouest", "Côte-des-Neiges-Notre-Dame-de-Grâce",
                                               "Westmount")) %>%
        group_by(f_neighbourhood_cleansed) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = rmse / mean_price
        )

c <- data_ne_holdout_w_prediction %>%
        filter(f_property_type %in% c("Entire condominium (condo)", "Entire loft")) %>%
        group_by(f_property_type) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = rmse / mean_price
        )

d <- data_ne_holdout_w_prediction %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = RMSE(predicted_price, price) / mean(price)
        )

# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("District", "", "", "")
result_subsamples <- rbind(line2, a, line1, c, line3, b, d) %>%
        transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
                  `RMSE/price` = as.numeric(`RMSE/price`))

```


## Further Research
In terms of external validity, we need to expect higher prediction errors if we change cities or times. We can further evaluate the prediction performance of our model on Montreal but for different past times. In this way we can have a better idea whether our model will perform well for the future. For different cities, I would expect larger prediction errors, as there are different features for different city and patterns with price are highly likely to be different than Montreal, therefore I would recommend to build models based on the data of that specific city to lower the prediction error.
