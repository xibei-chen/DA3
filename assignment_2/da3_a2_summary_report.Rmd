---
title: "Data Analysis 3 - Assignment 2"
author: "Xibei Chen"
date: "`r format(Sys.time(), '%d %B %Y')`"
subtitle: "Airbnb Price Prediction - Montreal, Canada"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, error=FALSE)
```

```{r, include=FALSE}
# Clear environment
rm(list=ls())

# Load packages
library(tidyverse)
library(data.table)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(cowplot)
library(modelsummary)
library(fixest)
library(ggthemes)
library(gridExtra)
library(rpart)
library(rattle)
library(rpart.plot)
library(ranger)
library(pdp)
library(gbm)
library(ggpubr)
library(kableExtra)

# Load raw data
data <- read_csv("/Users/xibei/Documents/2021CEU/DA3/listings_montreal_20211211_cleaned.csv") %>%
  mutate_if(is.character, factor) 
```


# Introduction
The purpose of this project is to help a company set price to their new apartments that are not yet on the rental market. The company is operating small and mid-size apartments hosting 2-6 guests in Montreal. To help them set reasonable price, my approach is to analyze the data of airbnb listings in Montreal on 11th Dec 2021 (the most recent date available) from [Inside Airbnb](http://insideairbnb.com/get-the-data.html). Then I build several price prediction models with methods, such as OLS, Lasso, and Random Forest. Eventually with prediction performance, interpretability, and external validity taken into consideration, I will decide which prediction model to use for helping the company set price to their apartments. (Please note that this is a summary report, please find more technical details on the technical report)

# Data Preparation 
Data preparation includes mainly three parts: Data Cleaning, Label Engineering, and Feature Engineering. For Data Cleaning, I filtered observations meeting the following criteria: with valid Airbnb's unique identifier for the listing, maximum capacity is between 2 and 6, room type is entire apartment or flat, property type is entire place (boat and RV excluded). I have also change format of some variables and handled missing values with different methods such as imputing, adding flag, etc.


### Label Engineering & EDA

```{r, echo=FALSE}
datasummary( price ~ Mean + Median + SD+ Min + P25 + P75 + Max + N, data = data, caption ="Descriptive Statistics of Daily Price (CA$)") %>% kable_styling(latex_options = c("HOLD_position","scale_down"))
```
Our target variable is the daily price in CA Dollars, which is very straightforward. The Mean and Median is not too far way from each other, but Mean is larger than Median, indicating there are some extreme values at right side. For now I decide not to exclude them, because there is no proof that they are errors.

### Feature Engineering
I create many dummy variables for amenities and host_verifications. And I created following variable groups for further analysis about variable importance. And I put the symbols such as "d_", "f_" "n_" and to indicate what type the variable is, dummy, factor or numeric. I have also added following features to capture potential non-linear patterns.: squared_number_of_reviews, cubic_number_of_reviews, ln_host_since_to_today. Besides, Interaction terms have also been created for modelling with OLS and Lasso, as I see that for different property types, the patterns between price and if pets allowed, and if there is pool change. In addition for different neighbourhoods, the patterns between price and if there is free parking and if there is lake access also change. (Here I did not create interaction terms with amenities, because there are so many, it would be super hard to compute because of the lack of computing power)

```{r, include=FALSE}
# Source a function (Credits to Agoston) -> to plot interaction terms
source("/Users/xibei/Documents/2021CEU/DA3/price_diff_by_variables2.R")

# Look up property type with pets allowed and pool
p1 <- price_diff_by_variables2(data, "f_property_type", "d_pets_allowed", "Property type", "Pets Allowed") + theme(axis.text.x = element_text(angle = 45, hjust=1))
p2 <- price_diff_by_variables2(data, "f_property_type", "d_pool", "Property type", "Pool") + theme(axis.text.x = element_text(angle = 45, hjust=1))
# Patterns change

# Look up neighbourhood with beachfront and lake access
p3 <- price_diff_by_variables2(data, "f_neighbourhood_cleansed", "d_free_parking", "Neighbourhood", "Free Parking")+ theme(axis.text.x = element_text(angle = 45, hjust=1))
p4 <- price_diff_by_variables2(data, "f_neighbourhood_cleansed", "d_lake_access", "Neighbourhood", "Lake Access")+ theme(axis.text.x = element_text(angle = 45, hjust=1))
# Patterns change
```


# Modelling

### Preparation

Mange different samples: Create a holdout set with 20% of observations by random sampling, and the left 80% would be used as work set; Create multiple predictor levels: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, amenities_general, and two interactions levels X1 and X2.

```{r, include=FALSE}
# Manage samples:
# Create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data))

# Set the random number generator to make results reproducible
set.seed(20220210)

# Create ids:
holdout_ids <- sample( seq_len( nrow( data ) ) , size = smp_size )
data$holdout <- 0
data$holdout[holdout_ids] <- 1

# Hold-out set Set
data_holdout <- data %>% filter(holdout == 1)

# Working data set
data_work <- data %>% filter(holdout == 0)
```


```{r, include=FALSE}
# Source from pre-created variable groups
source("/Users/xibei/Documents/2021CEU/DA3/define_var_groups.R")
```

```{r, include=FALSE}
# Basic levels
basic_lev  <- c("f_neighbourhood_cleansed", "f_property_type", "n_accommodates","n_bedrooms","n_beds", "n_dt_host_since_to_today")

# Basic Add levels
basic_add_lev <- c("n_minimum_nights", "n_maximum_nights", "d_has_availability", "n_availability_30", "d_instant_bookable")

# Reviews levels
reviews_lev <- c("n_number_of_reviews","n_review_scores_rating","n_review_scores_cleanliness", "n_review_scores_value")

# Polynomial and log-level 
poly_log_lev <- c("n_squared_number_of_reviews", "n_cubic_number_of_reviews", "n_ln_host_since_to_today")

# Amenities general levels - I grouped already previously
# amenities_general

# Interaction terms
X1  <- c("f_property_type*d_pets_allowed",  "f_property_type*d_pool")
X2  <- c("f_neighbourhood_cleansed*d_free_parking", "f_neighbourhood_cleansed*d_lake_access")
```


### (1) OLS 
I build following 7 models with increasing model complexity.

* Model 1: n_accommodates;
* Model 2: basic_lev;
* Model 3: basic_lev, basic_add_lev;
* Model 4: basic_lev, basic_add_lev, reviews_lev, poly_log_lev;
* Model 5: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1;
* Model 6: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1, X2;
* Model 7: basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1, X2, amenities_general.
```{r, include=FALSE}
# Create models 1-8 from simple to complex
modellev1 <- " ~ n_accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1,X2),collapse = " + "))
modellev7 <- paste0(" ~ ",paste(c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, X1,X2, amenities_general),collapse = " + "))


# Utilize the Working data set:
# Estimate measures on the whole working sample (R2,BIC,RMSE)
# Do K-fold cross validation to get proper Test RMSE

# Do everything within a for-loop
# K = 5
k_folds <- 5
# Define seed value
seed_val <- 20220210

# Do the iteration
for ( i in 1:7 ){
        print(paste0( "Estimating model: " ,i ))
        # Get the model name
        model_name <-  paste0("modellev",i)
        model_pretty_name <- paste0("M",i,"")
        # Specify the formula
        yvar <- "price"
        xvars <- eval(parse(text = model_name))
        formula <- formula(paste0(yvar,xvars))
        
        # Estimate model on the whole sample
        model_work_data <- feols( formula , data = data_work , vcov='hetero' )
        #  and get the summary statistics
        fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
        BIC <- fs$bic
        r2  <- fs$r2
        rmse_train <- fs$rmse
        ncoeff <- length( model_work_data$coefficients )
        
        # Do the k-fold estimation
        set.seed(seed_val)
        cv_i <- train( formula, data_work, method = "lm", 
                       trControl = trainControl(method = "cv", number = k_folds))
        rmse_test <- mean( cv_i$resample$RMSE )
        
        # Save the results
        model_add <- tibble(Model=model_pretty_name, Coefficients=ncoeff,
                            R_squared=r2, BIC = BIC, 
                            Training_RMSE = rmse_train, Test_RMSE = rmse_test )
        if ( i == 1 ){
                model_results <- model_add
        } else{
                model_results <- rbind( model_results , model_add )
        }
}
```


### (2) Lasso
```{r, include=FALSE}
# Set lasso tuning parameters:
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
# Take OLS model 7
formula <- formula(paste0("price", paste(modellev7, collapse = " + ")))

# Run LASSO
set.seed(seed_val)
lasso_model <- caret::train(formula,
                            data = data_work,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)
```


```{r, include=FALSE}
# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
        as.matrix() %>%
        as.data.frame() %>%
        rownames_to_column(var = "variable") %>%
        rename(coefficient = `s1`)  # the column has a name "1", to be renamed

#print(lasso_coeffs)

# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
        filter(coefficient!=0)
#print(nrow(lasso_coeffs_nz))
# Lasso picks 106 coefficients
```


```{r, include=FALSE}
# Get the RMSE of the Lasso model 
# Compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
        filter(lambda == lasso_model$bestTune$lambda) 

# Create an auxilary tibble
lasso_add <- tibble(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    R_squared=lasso_fitstats$Rsquared, BIC = NA, 
                    Training_RMSE = NA, Test_RMSE = lasso_fitstats$RMSE )
# Add it to final results
model_results_lasso <- rbind( model_results , lasso_add )
```

```{r, echo=FALSE}
kable(model_results_lasso, booktabs = TRUE, caption = 'OLS and Lasso Performance Summary (5-fold CV)') %>% kable_styling(latex_options = c("hold_position","scale_down"))
```


```{r, include=FALSE}
# Diagnostics
# Evaluate performance on the hold-out sample
# Re-run Model 7 on the work data
m7 <- feols( formula(paste0("price",modellev7)) , data = data_work, vcov = 'hetero' )

# Make prediction for the hold-out sample with M7 and Lasso
m7_p <- predict( m7 , newdata = data_holdout )
mL_p <- predict( lasso_model , newdata = data_holdout )

# Calculate the RMSE on hold-out sample
m7_rmse <- RMSE(m7_p,data_holdout$price)
mL_rmse <- RMSE(mL_p,data_holdout$price)

# Create a table
sum <- rbind(m7_rmse,mL_rmse)
rownames(sum) <- c('OLS M7','LASSO')
colnames(sum) <- c('RMSE on hold-out sample')
# Lasso also won Model 7 on hold-out sample by around 0.2$
```

For Lasso modelling, we still use the OLS M7 predictors. From the above summary table, we can see that M7 with 152 coefficients is the best among OLS Models 1-7. However, Lasso with `r nrow(lasso_coeffs_nz)` coefficients picked from M7 gets us slight better Test_RMSE than OLS M7 by around 0.2\$. I also evaluated the prediction performance of both OLS M7 and Lasso on our hold-out sample. OLS M7 with `r m7_rmse`, and Lasso `r mL_rmse`. Hence, Lasso also wins OLS M7 on hold-out set, by roughly 0.4\$.

```{r, echo=FALSE, fig.align='center', fig.width=6, fig.height=4} 
# Predicted vs Actual prices
data_holdout$predLp <- mL_p

ggplot( data_holdout , aes( y = price , x = predLp ) ) +
        geom_point( size = 1 , color = '#2a9d8f' , alpha=0.4) +
        geom_abline( intercept = 0, slope = 1, size = 1, color = '#e85d04' , alpha=0.8, linetype = 'dashed') +
        xlim(-1,max(data_holdout$price))+
        ylim(-1,max(data_holdout$price))+
        labs(x='Predicted Daily Price (CA$)',y='Acutal Daily Price (CA$)')+
        theme_minimal()
```
I created the above graph to visualize how Lasso performs prediction on hold-out sample. We can see that Lasso does not perform prediction well at extreme values of actual price, even though it is better than OLS models in general.

### (3) Random Forest
```{r, include=FALSE}
predictors_all <- colnames(data)
elements_2_remove <- c("price", "holdout")
predictors_all <- predictors_all[!(predictors_all %in% elements_2_remove)]

# Do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)

# Set tuning, 177 predictors, so set 13 variables per split, and arbitrarily set 50 min size per terminal node.
tune_grid <- expand.grid(
        .mtry = c(13),
        .splitrule = "variance",
        .min.node.size = c(50)
)

set.seed(seed_val)

rf_model <- train(
                formula(paste0("price ~", paste0(predictors_all, collapse = " + "))),
                data = data_work,
                method = "ranger",
                trControl = train_control,
                tuneGrid = tune_grid,
                importance = "impurity"
        )


```

Another method for prediction is regression tree, which can capture interactions and non-linearities automatically. However, it is prone to overfit data, even after pruning. Therefore, I choose to use Random Forest directly here after OLS and Lasso. With the method of Bootstrap Aggregation, 500 trees are created based on similar but not the exact same samples, and then the mean of predicted price is calculated. As I have `r length(predictors_all)` predictors in total, I arbitrarily set for each split, only randomly picked 13 variables (closest to square root of `r length(predictors_all)`) are taken into consideration. In this way, the trees are decorrelated kept independent from each other, and more chance is given to all the predictors. Both bootstrapping and decorrelating trees mean using random sets of information (observations and predictors). By tuning I also arbitrarily set 50 observations for minimum size per terminal node to avoid each tree overfitting to some extent. It turns out Random Forest does better prediction than both OLS and Lasso. In term of Holdout RMSE, Random forest with around 61\$ outperforms Lasso with around 64\$ by around 3\$, which is quite a good improvement.

### Model Performace Comparison
```{r, echo=FALSE}
M7predictors <- c(basic_lev, basic_add_lev, reviews_lev, poly_log_lev, amenities_general, X1, X2)

set.seed(20220210)
ols_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_work,
                method = "lm",
                trControl = train_control
        )
# ols_model_coeffs <-  ols_model$finalModel$coefficients
# ols_model_coeffs_df <- data.frame(
#         "variable" = names(ols_model_coeffs),
#         "ols_coefficient" = ols_model_coeffs
# ) %>%
#         mutate(variable = gsub("`","",variable))

# set.seed(20220210)
# lasso_model <- train(
#                 formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
#                 data = data_work,
#                 method = "glmnet",
#                 preProcess = c("center", "scale"),
#                 tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
#                 trControl = train_control
#         )
# 
# lasso_coeffs <- coef(
#         lasso_model$finalModel,
#         lasso_model$bestTune$lambda) %>%
#         as.matrix() %>%
#         as.data.frame() %>%
#         rownames_to_column(var = "variable") %>%
#         rename(lasso_coefficient = `s1`)
# 
# lasso_coeffs_non_null <- lasso_coeffs[!lasso_coeffs$lasso_coefficient == 0,]
# 
# regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_non_null, by = "variable", all=TRUE)


# Compare these models
final_models <-
        list("OLS (M7 predictors)" = ols_model,
             "LASSO (M7 predictors)" = lasso_model,
             "Random forest  (all predictors)" = rf_model)

results <- resamples(final_models) %>% summary()
#results

# Model selection is carried out on this CV RMSE
result_cv <- imap(final_models, ~{
        mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("CV RMSE" = ".")
# Random forest is the best

# Evaluate preferred model on the holdout set
result_holdout <- map(final_models, ~{
        RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("Holdout RMSE" = ".")

result_summary <- result_cv %>% cbind(result_holdout)
# Random forest is the best

kable(result_summary, booktabs = TRUE) %>% kable_styling(full_width= F, latex_options = c("hold_position","scale_down"))
```




# Robustness Check 
```{r, include=FALSE}
# Define extreme values 75th percentile + 1.5 * IQR = 226
extreme_price <- quantile(data$price, 0.75) + 1.5*(quantile(data$price, 0.75)-quantile(data$price, 0.25))
```

Even though random Forest does quite a good prediction job compared to OLS and Lasso, the RMSE on the hold-out set is still quite large, above 60\$. Recall that Lasso does poor job at prediction extreme values, leads me to think about whether excluding apartments with extreme values of price could make us get a model with better prediction. Therefore, I decided to exclude apartments with price higher than 75th percentile plus 1.5*IQR which is `r extreme_price`\$ to rerun the above three models. 
```{r, echo=FALSE}
# Since we are a company operating small and mid-size apartments hosting 2-6 guests, we are not supposed to be extreme values normally.
# So I decided to exclude the flat whose price is above 75th percentile + 1.5*IQR
data_ne <- data %>% filter(price <= extreme_price)


# Manage samples:
# Create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data_ne))

# Set the random number generator to make results reproducible
set.seed(20220210)

# Create ids:
holdout_ids <- sample( seq_len( nrow( data_ne) ) , size = smp_size )
data_ne$holdout <- 0
data_ne$holdout[holdout_ids] <- 1

# Hold-out set Set
data_ne_holdout <- data_ne %>% filter(holdout == 1)

# Working data set
data_ne_work <- data_ne %>% filter(holdout == 0)

# OLS
set.seed(20220210)
ne_ols_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_ne_work,
                method = "lm",
                trControl = train_control
        )

# Lasso
set.seed(20220210)
ne_lasso_model <- train(
                formula(paste0("price ~", paste0(M7predictors, collapse = " + "))),
                data = data_ne_work,
                method = "glmnet",
                preProcess = c("center", "scale"),
                tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
                trControl = train_control
        )


# Random forest
set.seed(20220210)

ne_rf_model <- train(
                formula(paste0("price ~", paste0(predictors_all, collapse = " + "))),
                data = data_ne_work,
                method = "ranger",
                trControl = train_control,
                tuneGrid = tune_grid,
                importance = "impurity"
        )



# Compare these models
ne_final_models <-
        list("OLS (M7 predictors)" = ne_ols_model,
             "LASSO (M7 predictors)" = ne_lasso_model,
             "Random forest  (all predictors)" = ne_rf_model)

ne_results <- resamples(ne_final_models) %>% summary()
#ne_results

# Model selection is carried out on this CV RMSE
ne_result_cv <- imap(ne_final_models, ~{
        mean(ne_results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("CV RMSE (without EV)" = ".")

# Evaluate preferred model on the holdout set
ne_result_holdout <- map(ne_final_models, ~{
        RMSE(predict(.x, newdata = data_ne_holdout), data_ne_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
        rename("Holdout RMSE (without EV)" = ".")

ne_result_summary <- ne_result_cv %>% cbind(ne_result_holdout)
# Random forest is the best
kable(ne_result_summary, booktabs = TRUE) %>% kable_styling(full_width= F, latex_options = c("hold_position","scale_down"))
```
We can tell from the above table that without extreme values taken into consideration, Random forest is still the best for both CV RMSE and Holdout RMSE. What is new is that the RMSE is only half of the RMSE compared to when we did not exclude extreme values earlier. This is huge improvement regarding prediction performance. Therefore, if the properties our company is operating has no unusual features, for example neighbor of celebrities, collections of famous paintings in the house, or any other luxurious facilities, if they are just normal properties, I would use the data without apartments with extreme values of price to train a Random Forest model for much better prediction. 


# External Validity
Random Forest gives better performance than OLS and Lasso at predicting quantitative target variable, but this comes at the cost. It is a black box, we do not have coefficients for hand-picked variables to interpret. Therefore, it is very hard to tell what patterns of association between daily price and the predictor variables look like. Besides, we are not aware of which variables are more important for the prediction. Patterns of association and variable important are extremely necessary for us to evaluate external validity. Therefore, I applied some diagnostic tools such as variable importance, partial dependence and performance across subsamples to dig deeper into the random forest to find out important variable and the pattern of association between them and price, and how performance varies among different subsamples. Here I keep using the data without extreme values of price.

### Variable Importance

```{r, echo=FALSE, fig.align='center', fig.dim=c(10,4)}
#### Variable importance plot
ne_rf_model_var_imp <- ranger::importance(ne_rf_model$finalModel)/1000 # scaled it down by 1000
ne_rf_model_var_imp_df <-
        data.frame(varname = names(ne_rf_model_var_imp),imp = ne_rf_model_var_imp) %>%
        mutate(varname = gsub("f_neighbourhood_cleansed", "District:", varname) ) %>%
        mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
        arrange(desc(imp)) %>%
        mutate(imp_percentage = imp/sum(imp))

# Full variable importance plot
#plot(varImp(ne_rf_model)) # too many variables, not clear

# Zoom in a bit
cutoff = 50
vi1 <- ggplot(ne_rf_model_var_imp_df[ne_rf_model_var_imp_df$imp>cutoff,],
       aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1.5) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha=0.6, size=1) +
        ylab("Importance (Percent)") +
        xlab("Variable Name") +
        coord_flip() +
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal() +
        theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
              axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))

# Top 10 important variable
vi2<-ggplot(ne_rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha = 0.6, size=0.75) +
        ylab("Importance (Percent)") +
        xlab("Variable Name") +
        coord_flip() +
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal()

# Grouped variable importance
availabilities_varnames <- grep("n_availability",colnames(data), value = TRUE)
minmax_nights_varnames <- grep("n_m",colnames(data), value = TRUE)
groups <- list(minmax_nights = minmax_nights_varnames,
               availability_days = availabilities_varnames,
               amenities_general = amenities_general,
               baby_children_friendly = baby_children_friendly,
               beauty_care_brands = beauty_care_brands,
               digital_entertainment = digital_entertainment,
               home_appliance_brands = home_appliance_brands,
               host_verifications = host_verifications,
               outdoor_facilities = outdoor_facilities,
               n_bathrooms = "n_bathrooms",
               n_accommodates = "n_accommodates",
               n_bedrooms = "n_bedrooms", 
               f_property_type = "f_property_type",
               f_neighbourhood_cleansed = "f_neighbourhood_cleansed"
               )

# Need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
        var.imp <- as.matrix(sapply(groups, function(g) {
                sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
        }))
        colnames(var.imp) <- "MeanDecreaseGini"
        return(var.imp)
}

ne_rf_model_var_imp_grouped <- group.importance(ne_rf_model$finalModel, groups)
ne_rf_model_var_imp_grouped_df <- data.frame(varname = rownames(ne_rf_model_var_imp_grouped),
                                            imp = ne_rf_model_var_imp_grouped[,1])  %>%
        mutate(imp_percentage = imp/sum(imp))

vi3 <- ggplot(ne_rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
        geom_point(color='#e85d04', size=1) +
        geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='#2a9d8f', alpha=0.6, size=0.7) +
        ylab("Importance (Percent)") +   xlab("Variable Name") +
        coord_flip() +
        # expand=c(0,0),
        scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
        theme_minimal()

ggarrange(vi2, vi3, ncol = 2, nrow = 1)
```

Variable Importance is a sum of gains in terms of MSE reduction by splits involving the variable. Since there will be too many variables on full variable importance plot, and it would be impossible to see clearly. Here I only zoom in to see only top 10 on the left side. Here we can see that number of bedrooms, the maximum capacity and the number of bathrooms are the top 3, which can pass our sanity check. On the right side, after grouping, we can see that the sum of importance of all the general amenities variables is actually quite large. And it is the same for minimum and maximum nights and availability variables. They are more importance than the original top 3 after grouping together the variables. Another thing we can see is that variable groups such as host verification, baby children friendly, outdoor facilities, digital entertainment and beauty care brands do have quite some effect on price prediction as groups.


### Partial Dependence

```{r, include=FALSE}
# Partial Dependence Plots 
# Number of accommodates
pdp_n_acc <- pdp::partial(ne_rf_model, pred.var = "n_accommodates", 
                          pred.grid = distinct_(data_ne_holdout, "n_accommodates"), 
                          train = data_ne_work)

# #pdp_n_acc %>%
#         autoplot( ) +
#         geom_line(color='#2a9d8f', size=1) +
#         geom_point(color='#e85d04', size=2) +
#         ylab("Predicted price") +
#         xlab("Accommodates (persons)") +
#         scale_x_continuous(limit=c(2,6), breaks=seq(2,6,1))+
#         theme_minimal()
```

```{r, include=FALSE}
# Property type
pdp_n_propertytype <- pdp::partial(ne_rf_model, pred.var = "f_property_type", 
                               pred.grid = distinct_(data_ne_holdout, "f_property_type"), 
                               train = data_ne_work)
# pdp_n_propertytype %>%
#         autoplot( ) +
#         geom_point(color='#e85d04', size=4) +
#         ylab("Predicted price") +
#         xlab("Property type") +
#         scale_y_continuous(limits=c(95,105), breaks=seq(95,105, by=5)) +
#         theme_minimal()+
#         theme(axis.text.x=element_text(angle=45,hjust=1)) 

```
Next I examine the pattern of association between price and some of the predictor variables. I picked two variables that I would like to know more about and check the pattern: the number of guests to accommodate and property type. It turns out there is a pretty linear and positive relationship between predicted price and number of guests. In terms of property types, entire townhouse, entire loft and entire residential home are the top 3 expensive types, whereas entire rental unit is the cheapest type.

### Performance on Subsamples
To further investigate the performance of the random forest model, I also look at subsamples by three predictor variables: apartment size, property types and districts. For apartment size, I divide apartments into two group, ones with 2 or 3 guests capacity, and other ones with 4-6 guests. It turns out prices of small size apartments are slightly harder to predict than medium size apartments. For property types, I arbitrarily pick two that I am more interested in, namely Entire condo and Entire loft. There is not much difference regarding predictive performance among these two property types. For districts, I choose 5 districts that are located in the inner city, where most investment properties are located. It turns out Côte-des-Neiges-Notre-Dame-de-Grâce and Westmount, the two districts at the west part of city center, are harder than other inner city districts to predict price. 
```{r, include=FALSE}
# Subsample performance
data_ne_holdout_w_prediction <- data_ne_holdout %>%
        mutate(predicted_price = predict(ne_rf_model, newdata = data_ne_holdout))

# Create nice summary table of heterogeneity
a <- data_ne_holdout_w_prediction %>%
        mutate(is_low_size = ifelse(n_accommodates <= 3, "Small apt", "Medium apt")) %>%
        group_by(is_low_size) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = RMSE(predicted_price, price) / mean(price)
        )

b <- data_ne_holdout_w_prediction %>%
        filter(f_neighbourhood_cleansed %in% c("Ville-Marie", "Le Plateau-Mont-Royal",
                                               "Le Sud-Ouest", "Côte-des-Neiges-Notre-Dame-de-Grâce",
                                               "Westmount")) %>%
        group_by(f_neighbourhood_cleansed) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = rmse / mean_price
        )

c <- data_ne_holdout_w_prediction %>%
        filter(f_property_type %in% c("Entire condominium (condo)", "Entire loft")) %>%
        group_by(f_property_type) %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = rmse / mean_price
        )

d <- data_ne_holdout_w_prediction %>%
        dplyr::summarise(
                rmse = RMSE(predicted_price, price),
                mean_price = mean(price),
                rmse_norm = RMSE(predicted_price, price) / mean(price)
        )

# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("District", "", "", "")
result_subsamples <- rbind(line2, a, line1, c, line3, b, d) %>%
        transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
                  `RMSE/price` = as.numeric(`RMSE/price`))

```


# Conclusion
Among 7 OLS models, Lasso and Random Forest, Random Forest has the best prediction performance. By robustness check, I found out by excluding apartments with extreme value prices, prediction performance can be improved significantly. So given that the properties of our company are normal apartments without extraordinary features, I would use Random Forest on data without extreme values of price for prediction. Besides, we should keep in mind that depending on the location and features the properties of our company, we might have different prediction performance. Last but not least, improving amenities, host verification, availability, and other features might enable us to set a relative higher price to generate higher profit for our business.
