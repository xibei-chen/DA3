% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\title{Data Analysis 3 - Assignment 2}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Airbnb Price Prediction - Montreal, Canada}
\author{Xibei Chen}
\date{09 February 2022}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Analysis 3 - Assignment 2},
  pdfauthor={Xibei Chen},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{siunitx}
\newcolumntype{d}{S[input-symbols = ()]}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The purpose of this project is to help a company set price to their new
apartments that are not yet on the rental market. The company is
operating small and mid-size apartments hosting 2-6 guests in Montreal.
To help them set reasonable price, my approach is to analyze the data of
airbnb listings in Montreal on 11th Dec 2021 (the most recent date
available) from \href{http://insideairbnb.com/get-the-data.html}{Inside
Airbnb} (a website providing fairly cleaned web scrapped data on
listings throughout all the major cities of the world) and then I build
several price prediction models based on the data with methods, such as
OLS, Lasso, and Random Forest. Eventually with prediction performance,
interpretability, external validity etc. taken into consideration, I
will decide which prediction model to use for helping the company set
price to their apartments.

\hypertarget{data-preparation}{%
\section{Data Preparation}\label{data-preparation}}

Data preparation includes mainly three parts: Data Cleaning, Label
Engineearing, and Feature Engineearing. (Please find more details on
\href{https://github.com/xibei-chen/DA3/blob/main/assignment_2/montreal_20211211_cleaner.R}{Cleaner
R script})

\hypertarget{data-cleaning}{%
\subsubsection{Data Cleaning}\label{data-cleaning}}

Filter Observations:

\begin{itemize}
\tightlist
\item
  With valid Airbnb's unique identifier for the listing;
\item
  The maximum capacity is between 2 and 6;
\item
  Room type is entire apartment or flat;
\item
  Property type is entire place (boat and RV excluded).
\end{itemize}

Change Format \& Handle missing values

\hypertarget{label-engineering-eda}{%
\subsubsection{Label Engineering \& EDA}\label{label-engineering-eda}}

Our target variable is the daily price in CA\$, which is very
straightforward.

\hypertarget{descriptive-statistics-of-daily-price}{%
\subparagraph{Descriptive Statistics of Daily
Price}\label{descriptive-statistics-of-daily-price}}

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrr}
\toprule
  & Mean & Median & SD & Min & P25 & P75 & Max & N\\
\midrule
price & \num{113.34} & \num{95.00} & \num{78.96} & \num{15.00} & \num{68.00} & \num{130.00} & \num{999.00} & 8390\\
\bottomrule
\end{tabular}}
\end{table}

Mean and Median is not too far way from each other, but Mean is larger
than Median, indicating there are some extreme values at right side,
which is verified by the following graph of distribution of daily price.
For now I decide not to exclude them, because there is no proof that
they are errors.

\begin{center}\includegraphics{da3_a2_summary_report_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{feature-engineering}{%
\subsubsection{Feature Engineering}\label{feature-engineering}}

I create many dummy variables for amenities and host\_verifications. And
I created following variable groups for further analysis about variable
importance. (Please find more details on
\href{https://github.com/xibei-chen/DA3/blob/main/assignment_2/define_var_groups.R}{Define
Variable Groups}) And I put the symbols such as ``d\_'', ``f\_'' ``n\_''
and to indicate what type the variable is, dummy, factor or numeric. I
have also added following features to capture potential non-linear
patterns.

\begin{itemize}
\tightlist
\item
  squared\_number\_of\_reviews;
\item
  cubic\_number\_of\_reviews;
\item
  ln\_host\_since\_to\_today.
\end{itemize}

Interaction terms below have also been created for modelling with OLS
and Lasso, as I see that for different property types, the patterns
between price and if pets allowed, and if there is pool change. In
addition for different neighbourhoods, the patterns between price and if
there is free parking and if there is lake access also change. (Here I
did not create interaction terms with amenities, because there are so
many, it would be super hard to compute because of the lack of computing
power)

\begin{itemize}
\tightlist
\item
  f\_property\_type*d\_pets\_allowed;
\item
  f\_property\_type*d\_pool;
\item
  f\_neighbourhood\_cleansed*d\_free\_parking;
\item
  f\_neighbourhood\_cleansed*d\_lake\_access.
\end{itemize}

\hypertarget{modelling}{%
\section{Modelling}\label{modelling}}

\hypertarget{preparation}{%
\subsubsection{Preparation}\label{preparation}}

\begin{itemize}
\tightlist
\item
  Mange different samples: Create a holdout set with 20\% of
  observations by random sampling, and the left 80\% would be used as
  work set;
\item
  Create multiple predictor levels: basic\_lev, basic\_add\_lev,
  reviews\_lev, poly\_log\_lev, amenities\_general, and two interactions
  levels X1 and X2.
\end{itemize}

\hypertarget{ols}{%
\subsubsection{(1) OLS}\label{ols}}

I build following 7 models with increasing model complexity.

\begin{itemize}
\tightlist
\item
  Model 1: n\_accommodates;
\item
  Model 2: basic\_lev;
\item
  Model 3: basic\_lev, basic\_add\_lev;
\item
  Model 4: basic\_lev, basic\_add\_lev, reviews\_lev, poly\_log\_lev;
\item
  Model 5: basic\_lev, basic\_add\_lev, reviews\_lev, poly\_log\_lev,
  X1;
\item
  Model 6: basic\_lev, basic\_add\_lev, reviews\_lev, poly\_log\_lev,
  X1, X2;
\item
  Model 7: basic\_lev, basic\_add\_lev, reviews\_lev, poly\_log\_lev,
  X1, X2, amenities\_general.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(model\_results, }\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{caption =} \StringTok{\textquotesingle{}OLS Model 1{-}7 Performance Summary (5{-}fold CV)\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{latex\_options =} \FunctionTok{c}\NormalTok{(}\StringTok{"hold\_position"}\NormalTok{,}\StringTok{"scale\_down"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-9}OLS Model 1-7 Performance Summary (5-fold CV)}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrr}
\toprule
Model & Coefficients & R\_squared & BIC & Training\_RMSE & Test\_RMSE\\
\midrule
M1 & 2 & 0.0767814 & 77285.11 & 76.47644 & 76.29090\\
M2 & 52 & 0.1757602 & 76964.52 & 72.26070 & 72.92222\\
M3 & 57 & 0.2213935 & 76626.29 & 70.23190 & 70.89325\\
M4 & 64 & 0.2267550 & 76641.60 & 69.98967 & 70.68750\\
M5 & 75 & 0.2322180 & 76690.94 & 69.74199 & 70.57547\\
\addlinespace
M6 & 114 & 0.2516933 & 76862.14 & 68.85178 & 70.32217\\
M7 & 152 & 0.2884578 & 76858.84 & 67.13913 & 69.13331\\
\bottomrule
\end{tabular}}
\end{table}

From the above summary table, we can see that in terms of BIC, M3 with
57 coefficients is the best. However, M7 with 152 coefficients is the
best according to R-squared, Training RMSE and Test RMSE, compared to
M6, M7 commits roughly 1.2\$ Test RMSE, which is quite a improvement.

\begin{center}\includegraphics{da3_a2_summary_report_files/figure-latex/unnamed-chunk-10-1} \end{center}

From the above graph, we can see that with the increase of OLS model
complexity and number of coefficients, both Training and Test RMSE are
dropping, which implies that our model might not yet overfit the
training data.

\hypertarget{lasso}{%
\subsubsection{(2) Lasso}\label{lasso}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(model\_results\_lasso, }\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{caption =} \StringTok{\textquotesingle{}OLS and Lasso Performance Summary (5{-}fold CV)\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{latex\_options =} \FunctionTok{c}\NormalTok{(}\StringTok{"hold\_position"}\NormalTok{,}\StringTok{"scale\_down"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-14}OLS and Lasso Performance Summary (5-fold CV)}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrr}
\toprule
Model & Coefficients & R\_squared & BIC & Training\_RMSE & Test\_RMSE\\
\midrule
M1 & 2 & 0.0767814 & 77285.11 & 76.47644 & 76.29090\\
M2 & 52 & 0.1757602 & 76964.52 & 72.26070 & 72.92222\\
M3 & 57 & 0.2213935 & 76626.29 & 70.23190 & 70.89325\\
M4 & 64 & 0.2267550 & 76641.60 & 69.98967 & 70.68750\\
M5 & 75 & 0.2322180 & 76690.94 & 69.74199 & 70.57547\\
\addlinespace
M6 & 114 & 0.2516933 & 76862.14 & 68.85178 & 70.32217\\
M7 & 152 & 0.2884578 & 76858.84 & 67.13913 & 69.13331\\
LASSO & 106 & 0.2492095 & NA & NA & 68.94066\\
\bottomrule
\end{tabular}}
\end{table}

For Lasso modelling, we still use the OLS M7 predictors. The elasticnet
mixing parameter can be set with 0 ≤ α ≤ 1. We can let machine run
different α to find the optimal one. However, to reduce the computing
time, here I set alpha=1, which is the lasso penalty. I let machine run
lamda from 0.05 to 1 by 0.05, to get the optimal. It turns out the
optimal lambda parameter lies at 0.45. Besides, we can get the number of
coefficients that Lasso picked, which is 106. From the summary table
above, we can tell that Lasso gets us slight better Test\_RMSE than OLS
M7 by around 0.2\$.

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lr}
\toprule
  & RMSE on hold-out sample\\
\midrule
OLS M7 & 64.32827\\
LASSO & 63.99183\\
\bottomrule
\end{tabular}}
\end{table}

I also evaluated the prediction performance of both OLS M7 and Lasso on
our hold-out sample. As we can see from above table, Lasso also wins OLS
M7, by roughly 0.4\$.

\begin{center}\includegraphics{da3_a2_summary_report_files/figure-latex/unnamed-chunk-17-1} \end{center}

I have also created the above graph to visualize how Lasso performs
prediction on hold-out sample. We can see that Lasso does not perform
prediction well at extreme values of actual price, even though it is
better than OLS models in general.

\hypertarget{random-forest}{%
\subsubsection{(3) Random Forest}\label{random-forest}}

Another method for prediction is regression tree, which can capture
interactions and non-linearities automatically. However, it is prone to
overfit data, even after pruning. Therefore, I choose to use Random
Forest directly here after OLS and Lasso. With the method of Bootstrap
Aggregation, 500 trees are created based on similar but not the exact
same samples, and then the mean of predicted price is calculated. As I
have 177 predictors in total, I arbitrarily set for each split, only
randomly picked 13 variables (closest to square root of 177) are taken
into consideration. In this way, the trees are decorrelated kept
independent from each other, and more chance is given to all the
predictors. Both bootstrapping and decorrelating trees mean using random
sets of information (observations and predictors). By tuning I also
arbitrarily set 50 observations for minimum size per terminal node to
avoid each tree overfitting to some extent.

\hypertarget{model-performace-comparison}{%
\subsubsection{Model Performace
Comparison}\label{model-performace-comparison}}

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrr}
\toprule
  & CV RMSE & Holdout RMSE\\
\midrule
OLS (M7 predictors) & 69.13331 & 64.32827\\
LASSO (M7 predictors) & 68.94066 & 63.99183\\
Random forest  (all predictors) & 66.16580 & 61.06271\\
\bottomrule
\end{tabular}}
\end{table}

From the above summary table, we can tell that Random Forest does better
prediction than both OLS and Lasso. In term of Holdout RMSE, Random
forest outperforms Lasso by around 3\$, which is quite a good
improvement.

\hypertarget{robustness-check}{%
\section{Robustness Check}\label{robustness-check}}

Even though random Forest does quite a good prediction job compared to
OLS and Lasso, the RMSE on the hold-out set is still quite large, above
60\$. Recall that Lasso does poor job at prediction extreme values,
leads me to think about whether excluding apartments with extreme values
of price could make us get a model with better prediction. Therefore, I
decided to exclude apartments with price higher than 75th percentile
plus 1.5*IQR which is 223\$ to rerun the above three models.

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrr}
\toprule
  & CV RMSE (without EV) & Holdout RMSE (without EV)\\
\midrule
OLS (M7 predictors) & 34.86540 & 35.12591\\
LASSO (M7 predictors) & 34.78986 & 35.01581\\
Random forest  (all predictors) & 33.16948 & 33.57070\\
\bottomrule
\end{tabular}}
\end{table}

We can tell from the above table that without extreme values taken into
consideration, Random forest is still the best for both CV RMSE and
Holdout RMSE. What is new is that the RMSE is only half of the RMSE
compared to when we did not exclude extreme values earlier. This is huge
improvement regarding prediction performance. Therefore, if the
properties our company is operating has no unusual features, for example
neighbor of celebrities, collections of famous paintings in the house,
or any other luxurious facilities, if they are just normal properties, I
would use the data without apartments with extreme values of price to
train a Random Forest model for much better prediction.

\hypertarget{external-validity}{%
\section{External Validity}\label{external-validity}}

Random Forest gives better performance than OLS and Lasso at predicting
quantitative target variable, but this comes at the cost. It is a black
box, we do not have coefficients for hand-picked variables to interpret.
Therefore, it is very hard to tell what patterns of association between
daily price and the predictor variables look like. Besides, we are not
aware of which variables are more important for the prediction. Patterns
of association and variable important are extremely necessary for us to
evaluate external validity. Therefore, I applied some diagnostic tools
such as variable importance, partial dependence and performance across
subsamples to dig deeper into the random forest to find out important
variable and the pattern of association between them and price, and how
performance varies among different subsamples. Here I keep using the
data without extreme values of price.

\hypertarget{variable-importance}{%
\subsubsection{Variable Importance}\label{variable-importance}}

Variable Importance is a sum of gains in terms of MSE reduction by
splits involving the variable.

\begin{center}\includegraphics{da3_a2_summary_report_files/figure-latex/unnamed-chunk-22-1} \end{center}

Since there will be too many variables on full variable importance plot,
and it would be impossible to see clearly. Here I only set cut off to
only show us some variables on the top in terms of variable importance
on the left side. And I zoom in to see only top 10 on the right side.
Here we can see that number of bedrooms, the maximum capacity and the
number of bathrooms are the top 3, which can pass our sanity check.
Plus, from the graph on the left we can see there are a lot variable
belonging to the same group, such as availability, minimum and maximum
nights, etc. To better understand how different variable groups are in
terms of importance, I grouped variables in the same category together,
such as availability, minimum and maximum nights, general amenities,
beauty care brands, digital entertainment, host verifications, etc.

\begin{center}\includegraphics{da3_a2_summary_report_files/figure-latex/unnamed-chunk-23-1} \end{center}

Before grouping variables, dishwasher is the only one factor among
amenities that lies on the top of variable importance plot. However,
after grouping, we can see that the sum of importance of all the general
amenities variables is actually quite large. And it is the same for
minimum and maximum nights and availability variables. They are more
importance than the original top 3 after grouping together the
variables. Another thing we can see is that although we barely saw
individual variables appears at the top variable importance chart, after
grouping up, variable groups such as host verification, baby children
friendly, outdoor facilities, digital entertainment and beauty care
brands do effect price prediction.

\hypertarget{partial-dependence}{%
\subsubsection{Partial Dependence}\label{partial-dependence}}

Next I examine the pattern of association between price and some of the
predictor variables. I picked two variables that I would like to know
more about and check the pattern: the number of guests to accommodate
and property type.

\begin{center}\includegraphics{da3_a2_summary_report_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{center}\includegraphics{da3_a2_summary_report_files/figure-latex/unnamed-chunk-25-1} \end{center}

The above plots suggests a pretty linear and positive relationship
between predicted price and number of guests. In terms of property
types, entire townhouse, entire loft and entire residential home are the
top 3 expensive types, whereas entire rental unit is the cheapest type.

\hypertarget{performance-on-subsamples}{%
\subsubsection{Performance on
Subsamples}\label{performance-on-subsamples}}

To further investigate the performance of the random forest model, I
also look at subsamples by three predictor variables: apartment size,
property types and districts. For apartment size, I divide apartments
into two group, ones with 2 or 3 guests capacity, and other ones with
4-6 guests. For property types, I arbitrarily pick two that I am more
interested in, namely Entire condo and Entire loft. For districts, I
choose 5 districts that are located in the inner city, where most
investment properties are located. Only calculating RMSE is not
meaningful, because RMSE is based on price difference, hence is by
definition lower when prices are lower. Therefore, I compared RMSE
values relative to the corresponding mean predicted price.

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrr}
\toprule
Var.1 & RMSE & Mean.price & RMSE.price\\
\midrule
Apartment size &  &  & \\
Medium apt & 35.16141 & 109.88934 & 0.3199711\\
Small apt & 32.08785 & 86.42822 & 0.3712659\\
Type &  &  & \\
Entire condominium (condo) & 36.93338 & 108.28405 & 0.3410786\\
\addlinespace
Entire loft & 36.56924 & 109.44211 & 0.3341423\\
District &  &  & \\
Côte-des-Neiges-Notre-Dame-de-Grâce & 36.15470 & 90.14754 & 0.4010614\\
Le Plateau-Mont-Royal & 33.80014 & 99.80342 & 0.3386672\\
Le Sud-Ouest & 32.75380 & 102.56522 & 0.3193461\\
\addlinespace
Ville-Marie & 34.21544 & 102.49163 & 0.3338364\\
Westmount & 54.80989 & 139.00000 & 0.3943157\\
All & 33.57070 & 97.47941 & 0.3443876\\
\bottomrule
\end{tabular}}
\end{table}

We can see from the above table that prices of small size apartments are
slightly harder to predict than medium size apartments. However, for
condo and loft there is not much difference regarding predictive
performance. Besides, Côte-des-Neiges-Notre-Dame-de-Grâce and Westmount,
the two districts at the west part of city center, are harder than other
inner city districts to predict price.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Among 7 OLS models, Lasso and Random Forest, Random Forest has the best
prediction performance. By robustness check, I found out by excluding
apartments with extreme value prices, prediction performance can be
improved significantly. So given that the properties of our company are
normal apartments without extraordinary features, I would use Random
Forest on data without extreme values of price for prediction. However,
we still keep in mind that depending on the location and features the
properties of our company, we might have different prediction
performance.

\end{document}
